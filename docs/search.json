[
  {
    "objectID": "dpp/workflows/api-extraction-workflow.html#process-steps",
    "href": "dpp/workflows/api-extraction-workflow.html#process-steps",
    "title": "API Extraction Workflow",
    "section": "2 Process Steps",
    "text": "2 Process Steps\n\n2.1 ERP Extraction Triggered\nThe workflow begins when an extraction request is initiated. This could happen:\n\nOn a scheduled basis (e.g., nightly extractions)\nOn-demand through user-initiated requests\nAs part of a larger data processing workflow\n\n\n\n2.2 Get Connection Parameters\nThe system retrieves the necessary connection parameters for the specified ERP system and client:\n\nAPI endpoint URLs\nConnection settings\nTimeout configurations\n\nThe system establishes a secure connection to the ERP API by:\n\nRetrieving secure credentials from the vault\nApplying the appropriate authentication method (API key, OAuth token, etc.)\nEstablishing the connection with the ERP API\nVerifying successful authentication\n\n\n\n2.3 Query Data Endpoints\nWith a successful connection established, the system queries the appropriate endpoints to extract the required data:\n\nSales orders\nSales order details\nInventory spec fields\nCustomer details\nVendor details\nProduct spec fields\n\n\n\n2.4 Transform to Standard Columns\nThe raw data from the ERP system is transformed to follow our standardized data model:\n\nColumn names are mapped to our standard naming conventions\nBasic data quality checks are performed\n\n\n\n2.5 Save to Parquet\nThe transformed data is converted to Parquet format:\n\nEfficient columnar storage format\nCompressed to save space\nOptimized for analytical queries\nSchema is preserved with appropriate data types\n\n\n\n2.6 Push to S3\nThe Parquet files are uploaded to our S3 storage: - Organized by client and data type - Stored with appropriate metadata - Secured with proper access controls - Made available for downstream processing",
    "crumbs": [
      "Distributor Partner Program",
      "Workflows",
      "API Extraction Workflow"
    ]
  },
  {
    "objectID": "dpp/core-business-use-cases.html",
    "href": "dpp/core-business-use-cases.html",
    "title": "Core Business Use Cases",
    "section": "",
    "text": "The system must be able to connect to various ERP systems (e.g., Epicor, Sage) via their APIs, authenticate, and extract required data types. This includes:\n\nAuthenticating with proper credentials (API keys, tokens, client certificates)\nBuilding appropriate requests for each ERP system\nHandling pagination and batching for large datasets\nManaging rate limits and connection timeouts\nCapturing and processing response data\nImplementing resilience patterns for API communication",
    "crumbs": [
      "Distributor Partner Program",
      "Core Business Use Cases"
    ]
  },
  {
    "objectID": "dpp/core-business-use-cases.html#extract-data-from-erp-systems-via-api",
    "href": "dpp/core-business-use-cases.html#extract-data-from-erp-systems-via-api",
    "title": "Core Business Use Cases",
    "section": "",
    "text": "The system must be able to connect to various ERP systems (e.g., Epicor, Sage) via their APIs, authenticate, and extract required data types. This includes:\n\nAuthenticating with proper credentials (API keys, tokens, client certificates)\nBuilding appropriate requests for each ERP system\nHandling pagination and batching for large datasets\nManaging rate limits and connection timeouts\nCapturing and processing response data\nImplementing resilience patterns for API communication",
    "crumbs": [
      "Distributor Partner Program",
      "Core Business Use Cases"
    ]
  },
  {
    "objectID": "dpp/core-business-use-cases.html#extract-data-from-erp-databases",
    "href": "dpp/core-business-use-cases.html#extract-data-from-erp-databases",
    "title": "Core Business Use Cases",
    "section": "2 Extract Data from ERP Databases",
    "text": "2 Extract Data from ERP Databases\nFor ERP systems that allow or require direct database access, the system must:\n\nConnect securely to various database types\nExecute appropriate queries for different data types\nHandle connection pooling and resource management\nApply appropriate filtering for incremental extraction\nProcess result sets efficiently\nRelease database resources properly",
    "crumbs": [
      "Distributor Partner Program",
      "Core Business Use Cases"
    ]
  },
  {
    "objectID": "dpp/core-business-use-cases.html#transform-extracted-data",
    "href": "dpp/core-business-use-cases.html#transform-extracted-data",
    "title": "Core Business Use Cases",
    "section": "3 Transform Extracted Data",
    "text": "3 Transform Extracted Data\nAll extracted data must undergo initial transformation to:\n\nStandardize column names according to our global data dictionary\nValidate data against defined schemas and quality rules\nConvert to Parquet format for efficient storage and processing\nApply optional compression for reduced storage needs\nIdentify and mask sensitive information when required\nPreserve data lineage information",
    "crumbs": [
      "Distributor Partner Program",
      "Core Business Use Cases"
    ]
  },
  {
    "objectID": "dpp/core-business-use-cases.html#secure-credential-management",
    "href": "dpp/core-business-use-cases.html#secure-credential-management",
    "title": "Core Business Use Cases",
    "section": "4 Secure Credential Management",
    "text": "4 Secure Credential Management\nThe system must securely manage credentials for various ERP systems:\n\nRetrieve credentials from HashiCorp Vault using least privilege access\nSupport different credential types (API keys, database credentials, certificates)\nHandle credential rotation and expiration\nEnsure credentials are never logged or persisted outside secure storage\nApply proper authentication mechanisms for each ERP type",
    "crumbs": [
      "Distributor Partner Program",
      "Core Business Use Cases"
    ]
  },
  {
    "objectID": "dpp/core-business-use-cases.html#track-data-lineage",
    "href": "dpp/core-business-use-cases.html#track-data-lineage",
    "title": "Core Business Use Cases",
    "section": "5 Track Data Lineage",
    "text": "5 Track Data Lineage\nFor audit and compliance purposes, the system must:\n\nTrack the source, extraction time, and parameters of each data extraction\nRecord transformation details and any quality issues\nLink extracted data to its final storage location\nProvide metrics on extraction volume and timing\nEnable traceability for data governance",
    "crumbs": [
      "Distributor Partner Program",
      "Core Business Use Cases"
    ]
  },
  {
    "objectID": "dpp/core-business-use-cases.html#monitor-and-report-system-health",
    "href": "dpp/core-business-use-cases.html#monitor-and-report-system-health",
    "title": "Core Business Use Cases",
    "section": "6 Monitor and Report System Health",
    "text": "6 Monitor and Report System Health\nThe system must provide comprehensive monitoring:\n\nReport on successful and failed extractions\nExpose metrics for performance and throughput\nAlert on critical failures or data quality issues\nLog detailed information for troubleshooting\nProvide health check endpoints for operational status",
    "crumbs": [
      "Distributor Partner Program",
      "Core Business Use Cases"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design-v3.html",
    "href": "archive/erp-extractor-design-v3.html",
    "title": "Design Document for Distributor Data Extraction Ingestion (Version 3)",
    "section": "",
    "text": "This design document covers the extraction ingestion component of our ETL pipeline. In our architecture, distributors integrate with our system via two primary methods:\n\nDirect Extraction Ingestion: Where our system automatically provisions the correct resources based solely on the three supplied parameters: --erp-type, --client-id, and --data-type.\nSFTP Ingestion: Where distributors SFTP their data to our environment (this method is handled separately).\n\nThe design detailed here pertains only to the direct extraction ingestion method. This process extracts distributor data and deposits it into a pre-dropzone S3 bucket. At this stage, a minor transformation is applied: column names are standardized according to our global data dictionary and the output files are converted to Parquet format. More complex transformations occur later in the pipeline.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion (Version 3)"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design-v3.html#overview",
    "href": "archive/erp-extractor-design-v3.html#overview",
    "title": "Design Document for Distributor Data Extraction Ingestion (Version 3)",
    "section": "",
    "text": "This design document covers the extraction ingestion component of our ETL pipeline. In our architecture, distributors integrate with our system via two primary methods:\n\nDirect Extraction Ingestion: Where our system automatically provisions the correct resources based solely on the three supplied parameters: --erp-type, --client-id, and --data-type.\nSFTP Ingestion: Where distributors SFTP their data to our environment (this method is handled separately).\n\nThe design detailed here pertains only to the direct extraction ingestion method. This process extracts distributor data and deposits it into a pre-dropzone S3 bucket. At this stage, a minor transformation is applied: column names are standardized according to our global data dictionary and the output files are converted to Parquet format. More complex transformations occur later in the pipeline.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion (Version 3)"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design-v3.html#system-architecture",
    "href": "archive/erp-extractor-design-v3.html#system-architecture",
    "title": "Design Document for Distributor Data Extraction Ingestion (Version 3)",
    "section": "2 System Architecture",
    "text": "2 System Architecture\n\n2.1 Entry Point and Application Hosting\n\nProgram Class:\n\nMain Method:\n\nParses command-line options (e.g., --erp-type, --client-id, --data-type) using System.CommandLine.\nSets up the dependency injection container and configures services.\nResolves the primary extraction ingestion service (ERPService) from the DI container and triggers the extraction process.\n\nCreateHostBuilder Method:\n\nConfigures the host with HashiCorp Vault, FerretDB services, core providers, and builder components.\nConfigures IHttpClientFactory with resilience policies for API-based integrations.\nSets up feature flags, metrics collection, and health checks.\n\n\n\n\n\n2.2 Extraction Ingestion Orchestration\n\nERPService Class:\n\nActs as the orchestrator for the extraction ingestion process.\nRetrieves credentials from HashiCorp Vault and configuration from FerretDB, respectively.\nDynamically resolves components for data extraction based on ERP type using factory functions.\nUses IHttpClientFactory for managing HTTP connections in API mode.\nSupports two extraction modes:\n\nAPI Mode: For ERPs that expose APIs.\nDatabase Mode: For ERPs that require direct database access.\n\nSupports both batch processing and incremental extraction with change data capture (CDC).\nApplies a minimal transform:\n\nStandardizes column names according to our global data dictionary.\nApplies data quality validations against predefined rules.\nConverts all files to Parquet format with optional compression.\n\nDeposits the extracted (and minimally transformed) data into a pre-dropzone S3 bucket.\nTracks data lineage for audit and compliance purposes.\n(Note: Subsequent, more complex transformations are performed later in the overall ETL pipeline.)\n\n\n\n\n2.3 External Dependencies\n\nInfrastructure Services:\n\nIVaultService: Retrieves ERP credentials from HashiCorp Vault with least privilege access.\nIFerretDBService: Fetches ERP configuration data from FerretDB.\nIAmazonS3: Uploads the extracted data into the pre-dropzone S3 bucket.\nIFeatureFlagService: Manages feature flags for gradual rollout of functionality.\nICatalogService: Integrates with data catalog for metadata management.\n\n.NET Libraries:\n\nSystem.CommandLine: For command-line parsing.\nMicrosoft.Extensions.Hosting & DI: For hosting and dependency injection.\nSystem.Text.Json: For JSON serialization/deserialization.\nIHttpClientFactory: For managing HttpClient instances in API extraction mode.\nVaultSharp: For interacting with HashiCorp Vault.\nMongoDB.Driver: For FerretDB interactions (FerretDB uses MongoDB wire protocol).\nPolly: For resilience patterns including circuit breakers and bulkheads.\nOpenTelemetry: For distributed tracing and metrics.\nFluentValidation: For data contract validation.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion (Version 3)"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design-v3.html#design-patterns-and-advanced-resilience-strategies",
    "href": "archive/erp-extractor-design-v3.html#design-patterns-and-advanced-resilience-strategies",
    "title": "Design Document for Distributor Data Extraction Ingestion (Version 3)",
    "section": "3 Design Patterns and Advanced Resilience Strategies",
    "text": "3 Design Patterns and Advanced Resilience Strategies\n\n3.1 Dependency Injection (DI)\n\nUsage:\n\nDecouples service construction from business logic.\nRegisters Vault client, FerretDB connection, providers, factory functions, builder components, and the main extraction service in the DI container.\nUses factory functions to dynamically resolve implementations based on runtime parameters.\nConfigures IHttpClientFactory and related services.\n\nBenefits:\n\nEnhances testability and maintainability.\nPromotes separation of concerns.\nLeverages built-in .NET capabilities without custom abstractions.\n\n\n\n\n3.2 Factory Function Approach\n\nComponents:\n\nRegistered factory functions for dynamic resolution: Func&lt;string, IExtractor&gt;, Func&lt;string, string, ITransformer&gt;, Func&lt;string, IDataUploader&gt;.\n\nUsage:\n\nCentralizes lookup logic directly in the DI container.\nDynamically resolves connectors, extractors, transformers, and uploaders based on ERP type or data type at runtime.\n\nBenefits:\n\nSimplifies implementation by leveraging built-in .NET DI capabilities.\nEliminates custom registry classes while maintaining the dynamic resolution capability.\nProvides clear, testable dependency paths.\nMakes component selection logic more transparent.\n\n\n\n\n3.3 Abstract Factory Pattern\n\nComponent:\n\nFactory functions registered in the DI container.\nIHttpClientFactory for HTTP client management.\n\nUsage:\n\nEncapsulates creation of ERP-specific components.\n\nBenefits:\n\nSupports multiple ERP systems with varying implementations without altering extraction logic.\n\n\n\n\n3.4 Builder Pattern\n\nComponents:\n\nAPIRequestBuilder & AuthenticationBuilder:\n\nProvide fluent interfaces to construct complex API request objects.\n\nDatabaseQueryBuilder:\n\nDynamically constructs SQL queries for ERPs requiring direct database access.\n\nExtractConfigBuilder:\n\nBuilds extraction configurations supporting both full and incremental extracts.\n\n\nUsage:\n\nThe DatabaseQueryBuilder collects parameters and produces a DatabaseQuery object with a GenerateSql method.\nThe APIRequestBuilder works with IHttpClientFactory to construct properly configured HTTP requests.\nThe ExtractConfigBuilder creates configurations for different extraction modes.\n\nBenefits:\n\nEnhances readability and modularity.\nSupports multiple extraction modes seamlessly.\n\n\n\n\n3.5 Strategy Pattern\n\nComponents:\n\nInterfaces such as IExtractor, ITransformer, and IValidator.\n\nUsage:\n\nEncapsulate different implementations for data extraction, transformation, and validation.\nFactory functions select the appropriate strategy at runtime.\n\nBenefits:\n\nProvides flexibility to extend or change algorithms without impacting the overall system.\n\n\n\n\n3.6 Decorator Pattern\n\nComponents:\n\nVaultCredentialProviderDecorator:\n\nAdds caching behavior to Vault credential retrieval.\n\nMetricsDecorator:\n\nWraps core services to collect performance metrics.\n\nEncryptionDecorator:\n\nAdds field-level encryption for sensitive data.\n\nDataQualityDecorator:\n\nAdds data quality checks to transformations.\n\n\nUsage:\n\nEnhances existing components with additional functionality without modifying their core logic.\n\nBenefits:\n\nImproves performance through caching.\nProvides observability and enhances security.\nEnsures data quality through validation.\n\n\n\n\n3.7 Circuit Breaker Pattern\n\nComponents:\n\nVaultCircuitBreaker, FerretDBCircuitBreaker, S3CircuitBreaker\n\nUsage:\n\nPrevents cascading failures by breaking the circuit when dependencies fail.\nAutomatically restores service when dependencies recover.\n\nBenefits:\n\nEnhances system resilience during partial outages.\nPrevents overwhelming failing services with requests.\n\n\n\n\n3.8 Bulkhead Pattern\n\nComponents:\n\nSeparate connection pools for different ERP types and operations.\nIsolated resource pools for critical vs. non-critical operations.\n\nUsage:\n\nIsolates failures to prevent system-wide degradation.\nAllocates resources based on operation criticality.\n\nBenefits:\n\nPrevents resource exhaustion.\nImproves system stability during partial failures.\n\n\n\n\n3.9 Exponential Backoff with Jitter\n\nUsage:\n\nImplements intelligent retry strategies for all external service calls.\nAdds randomization to prevent thundering herd problems.\n\nBenefits:\n\nPrevents overwhelming recovering services.\nDistributes retry attempts evenly over time.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion (Version 3)"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design-v3.html#detailed-component-descriptions",
    "href": "archive/erp-extractor-design-v3.html#detailed-component-descriptions",
    "title": "Design Document for Distributor Data Extraction Ingestion (Version 3)",
    "section": "4 Detailed Component Descriptions",
    "text": "4 Detailed Component Descriptions\n\n4.1 Program Class\n\nMain Method:\n\nParses Nomad-supplied command-line arguments.\nBuilds the DI container via CreateHostBuilder.\nResolves and invokes ERPService.ProcessERPData.\nHandles global errors for graceful failure.\n\nCreateHostBuilder Method:\n\nConfigures services including Vault client, FerretDB connection, providers, and builder components.\nRegisters factory functions for dynamic component resolution.\nSets up resilience policies with circuit breakers and bulkheads.\nConfigures feature flags, metrics collection, and health checks.\nSets up OpenAPI documentation generation.\n\n\n\n\n4.2 ERPService Class\n\nResponsibilities:\n\nOrchestrates the extraction ingestion process.\nRetrieves credentials and ERP configuration.\nDynamically resolves ERP-specific components using injected factory functions.\nChooses between API or Database extraction modes based on the ERP configuration.\nSupports both batch and incremental extraction modes.\nApplies a minor transformation with data quality validation.\nDeposits the transformed data into a pre-dropzone S3 bucket.\nTracks data lineage for audit and compliance.\n\nKey Method – ProcessERPData:\n\nCredential & Configuration Retrieval:\n\nUses ICredentialProvider to obtain short-lived, least-privilege credentials.\nUses IConfigurationProvider to obtain ERP settings.\n\nDynamic Component Resolution:\n\nUses factory functions to resolve ERP-specific extractors, transformers, and uploaders.\n\nIntegration Modes:\n\nAPI Mode:\n\nBuilds an API request via APIRequestBuilder with mutual TLS if supported.\nExtracts data via IExtractor.Extract with circuit breaker protection.\n\nDatabase Mode:\n\nBuilds a SQL query using DatabaseQueryBuilder.\nExtracts data via IExtractor.ExtractFromDatabase with connection isolation.\n\n\nExtraction Modes:\n\nFull Extract: Retrieves all data for the specified type.\nIncremental Extract: Retrieves only changed data since the last extraction.\n\nSelf-Healing:\n\nImplements automatic recovery procedures for common failure scenarios.\n\nSubsequent Steps:\n\nApplies the minor transform with data quality validation.\nRecords data lineage metadata.\nUploads the resulting data into a pre-dropzone S3 bucket via IDataUploader.\n\n\n\n\n\n4.3 Providers\n\nVaultCredentialProvider:\n\nRetrieves and deserializes credentials from HashiCorp Vault.\nImplements caching with TTL-based invalidation.\nSupports dynamic secret rotation with configurable TTL.\nGenerates least-privilege, operation-specific credentials.\n\nFerretDBConfigProvider:\n\nFetches ERP configuration from FerretDB and maps it to an ERPConfiguration object.\nUses MongoDB driver since FerretDB implements MongoDB wire protocol.\nSupports read-preference strategies for replica sets.\nImplements schema evolution for backward compatibility.\n\n\n\n\n4.4 Data Uploader – S3DataUploader\n\nResponsibilities:\n\nFormats and uploads the minimally transformed data into S3.\nApplies data compression for efficient storage and transfer.\nConverts files to Parquet format and ensures standardized column names.\nEncrypts sensitive fields before upload.\nRecords data lineage metadata.\n\nKey Methods:\n\nUpload: Manages the upload process with checksums and integrity verification.\nFormatData: Applies the transformation with data quality checks.\nTrackLineage: Records data provenance information.\n\n\n\n\n4.5 Data Quality and Validation\n\nDataContractValidator:\n\nResponsibilities:\n\nValidates data against predefined schemas and rules.\nReports quality issues with detailed diagnostics.\nEnforces data governance policies.\n\nKey Methods:\n\nValidateSchema: Ensures data adheres to expected structure.\nValidateValues: Checks data values against business rules.\nGenerateReport: Creates detailed validation reports.\n\n\nDataMaskingService:\n\nResponsibilities:\n\nIdentifies and masks sensitive information (PII).\nSupports various masking techniques (hashing, tokenization, etc.).\nMaintains referential integrity across masked datasets.\n\nKey Methods:\n\nIdentifySensitiveFields: Automatically detects potential PII.\nApplyMasking: Applies appropriate masking techniques.\nVerifyMasking: Ensures masking effectiveness.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion (Version 3)"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design-v3.html#pseudo-code-for-nomad-integration",
    "href": "archive/erp-extractor-design-v3.html#pseudo-code-for-nomad-integration",
    "title": "Design Document for Distributor Data Extraction Ingestion (Version 3)",
    "section": "5 Pseudo-code for Nomad Integration",
    "text": "5 Pseudo-code for Nomad Integration\n/// &lt;summary&gt;\n/// Configures and builds the host with all necessary services.\n/// &lt;/summary&gt;\nCreateHostBuilder:\n    return HostBuilder()\n        .ConfigureServices(services =&gt;\n        {\n            // Configure Vault client with circuit breaker\n            services.AddSingleton&lt;IVaultClient&gt;(provider =&gt;\n            {\n                var vaultOptions = new VaultClientSettings(\n                    \"https://vault.example.com:8200\",\n                    new AppRoleAuthMethodInfo(roleId, secretId)\n                );\n                vaultOptions.RetrySettings = new RetrySettings { \n                    Enabled = true, \n                    MaxAttempts = 5,\n                    BackoffType = BackoffType.ExponentialWithJitter\n                };\n                return new VaultClient(vaultOptions);\n            });\n\n            // Configure FerretDB connection with bulkhead isolation\n            services.AddSingleton&lt;IMongoClient&gt;(provider =&gt;\n            {\n                var settings = MongoClientSettings.FromConnectionString(\n                    \"mongodb://ferretdb.example.com:27017\"\n                );\n                settings.RetryWrites = true;\n                settings.RetryReads = true;\n                settings.ServerSelectionTimeout = TimeSpan.FromSeconds(5);\n                settings.MaxConnectionPoolSize = 100;\n                return new MongoClient(settings);\n            });\n\n            // Configure feature flag service\n            services.AddSingleton&lt;IFeatureFlagService, FeatureFlagService&gt;();\n\n            // Register providers with caching decorators\n            services.AddSingleton&lt;ICredentialProvider, VaultCredentialProvider&gt;();\n            services.Decorate&lt;ICredentialProvider, CachedCredentialProviderDecorator&gt;();\n            \n            services.AddSingleton&lt;IConfigurationProvider, FerretDBConfigProvider&gt;();\n            \n            // Register data quality and validation services\n            services.AddSingleton&lt;IDataContractValidator, DataContractValidator&gt;();\n            services.AddSingleton&lt;IDataMaskingService, DataMaskingService&gt;();\n            \n            // Register data lineage service\n            services.AddSingleton&lt;IDataLineageService, DataLineageService&gt;();\n            \n            // Register concrete implementations\n            services.AddSingleton&lt;EpicorExtractor&gt;();\n            services.AddSingleton&lt;SageExtractor&gt;();\n            services.AddSingleton&lt;EpicorTransformer&gt;();\n            services.AddSingleton&lt;SageTransformer&gt;();\n            services.AddSingleton&lt;S3DataUploader&gt;();\n            services.AddSingleton&lt;LocalFileUploader&gt;();\n            \n            // Register factory functions for dynamic resolution\n            services.AddSingleton&lt;Func&lt;string, IExtractor&gt;&gt;(sp =&gt; erpType =&gt; \n                erpType switch {\n                    \"Epicor\" =&gt; sp.GetRequiredService&lt;EpicorExtractor&gt;(),\n                    \"Sage\" =&gt; sp.GetRequiredService&lt;SageExtractor&gt;(),\n                    _ =&gt; throw new ArgumentException($\"Unknown ERP type: {erpType}\")\n                });\n                \n            services.AddSingleton&lt;Func&lt;string, string, ITransformer&gt;&gt;(sp =&gt; (erpType, dataType) =&gt; \n                (erpType, dataType) switch {\n                    (\"Epicor\", _) =&gt; sp.GetRequiredService&lt;EpicorTransformer&gt;(),\n                    (\"Sage\", _) =&gt; sp.GetRequiredService&lt;SageTransformer&gt;(),\n                    _ =&gt; throw new ArgumentException($\"Unsupported combination: {erpType}, {dataType}\")\n                });\n                \n            services.AddSingleton&lt;Func&lt;string, IDataUploader&gt;&gt;(sp =&gt; uploaderType =&gt; \n                uploaderType switch {\n                    \"s3\" =&gt; sp.GetRequiredService&lt;S3DataUploader&gt;(),\n                    \"local\" =&gt; sp.GetRequiredService&lt;LocalFileUploader&gt;(),\n                    _ =&gt; throw new ArgumentException($\"Unknown uploader type: {uploaderType}\")\n                });\n            \n            // Register builders\n            services.AddSingleton&lt;IAPIRequestBuilder, APIRequestBuilder&gt;();\n            services.AddSingleton&lt;IDatabaseQueryBuilder, DatabaseQueryBuilder&gt;();\n            services.AddSingleton&lt;IAuthenticationBuilder, AuthenticationBuilder&gt;();\n            services.AddSingleton&lt;IExtractConfigBuilder, ExtractConfigBuilder&gt;();\n            \n            // Register S3 client for data upload\n            services.AddAWSService&lt;IAmazonS3&gt;();\n            \n            // Register core service with decorators for cross-cutting concerns\n            services.AddSingleton&lt;ERPService&gt;();\n            services.Decorate&lt;ERPService, MetricsERPServiceDecorator&gt;();\n            services.Decorate&lt;ERPService, DataQualityDecorator&gt;();\n            services.Decorate&lt;ERPService, EncryptionDecorator&gt;();\n            \n            // Configure HTTP clients with resilience policies using Polly\n            services.AddHttpClient(\"default\")\n                .AddTransientHttpErrorPolicy(builder =&gt; \n                    builder.WaitAndRetryAsync(\n                        retryCount: 3, \n                        sleepDurationProvider: retryAttempt =&gt; \n                            TimeSpan.FromSeconds(Math.Pow(2, retryAttempt)) + \n                            TimeSpan.FromMilliseconds(new Random().Next(0, 1000)), // Jitter\n                        onRetry: (outcome, timespan, retryAttempt, context) =&gt; {\n                            // Log retry attempt\n                            logger.LogWarning($\"Retry {retryAttempt} for {context.PolicyKey} after {timespan.TotalSeconds}s delay\");\n                        }\n                    ))\n                .AddCircuitBreakerPolicy(builder =&gt;\n                    builder.CircuitBreakerAsync(\n                        handledEventsAllowedBeforeBreaking: 5,\n                        durationOfBreak: TimeSpan.FromSeconds(30),\n                        onBreak: (outcome, breakDelay) =&gt; {\n                            logger.LogError($\"Circuit broken for {breakDelay.TotalSeconds}s!\");\n                        },\n                        onReset: () =&gt; {\n                            logger.LogInformation(\"Circuit reset!\");\n                        }\n                    ));\n                \n            // Add OpenTelemetry tracing\n            services.AddOpenTelemetryTracing(builder =&gt; {\n                builder\n                    .SetResourceBuilder(ResourceBuilder.CreateDefault().AddService(\"erp-extractor\"))\n                    .AddSource(\"erp-extractor\")\n                    .AddHttpClientInstrumentation()\n                    .AddMongoDBInstrumentation()\n                    .AddAspNetCoreInstrumentation()\n                    .AddJaegerExporter();\n            });\n        });\n\n/// &lt;summary&gt;\n/// Processes ERP data extraction and performs initial transformation.\n/// &lt;/summary&gt;\n/// &lt;param name=\"erpType\"&gt;The type of ERP system to extract from&lt;/param&gt;\n/// &lt;param name=\"clientId\"&gt;The client identifier&lt;/param&gt;\n/// &lt;param name=\"dataType\"&gt;The type of data to extract&lt;/param&gt;\nERPService.ProcessERPData(erpType, clientId, dataType):\n    Log \"Starting ETL extraction ingestion for client [clientId] using ERP [erpType]\"\n\n    /// &lt;summary&gt;Start metrics collection for this operation&lt;/summary&gt;\n    using (metricsTimer = MetricsService.StartTimer(\"erp_process_data\", \n                                                  { \"erp_type\": erpType, \"client_id\": clientId }))\n    using (tracer = TracingService.StartTrace(\"ProcessERPData\"))\n    {\n        /// &lt;summary&gt;Start data lineage tracking&lt;/summary&gt;\n        lineage = DataLineageService.StartLineageRecord(erpType, clientId, dataType)\n        \n        /// &lt;summary&gt;Check feature flags for enabled features&lt;/summary&gt;\n        bool useIncrementalExtract = FeatureFlagService.IsFeatureEnabled(\"IncrementalExtract\", clientId)\n        bool useCompression = FeatureFlagService.IsFeatureEnabled(\"Compression\", clientId)\n        bool useFieldEncryption = FeatureFlagService.IsFeatureEnabled(\"FieldEncryption\", clientId)\n    \n        /// &lt;summary&gt;Retrieve least-privilege credentials from HashiCorp Vault&lt;/summary&gt;\n        credentials = CredentialProvider.GetLeastPrivilegeCredentials(erpType, clientId, dataType)\n\n        /// &lt;summary&gt;Retrieve configuration from FerretDB&lt;/summary&gt;\n        erpConfig = ConfigurationProvider.GetConfiguration(erpType, clientId)\n\n        /// &lt;summary&gt;\n        /// Dynamically resolve ERP-specific components using factory functions:\n        /// - Data extractor (for API or DB extraction)\n        /// - Data transformer (to standardize columns and convert to Parquet)\n        /// - Data uploader (to upload data to the pre-dropzone S3 bucket)\n        /// &lt;/summary&gt;\n        extractor = ExtractorFactory(erpType);\n        transformer = TransformerFactory(erpType, dataType);\n        uploader = UploaderFactory(\"s3\");\n\n        /// &lt;summary&gt;Build extraction configuration based on mode&lt;/summary&gt;\n        extractConfig = ExtractConfigBuilder.New()\n            .ForERP(erpType)\n            .ForClient(clientId)\n            .ForDataType(dataType)\n            .UseIncrementalExtract(useIncrementalExtract && erpConfig.SupportsCDC)\n            .WithLastExtractTime(useIncrementalExtract ? GetLastExtractTime(erpType, clientId, dataType) : null)\n            .WithBatchSize(erpConfig.BatchSize)\n            .Build()\n\n        /// &lt;summary&gt;Handle database extraction mode&lt;/summary&gt;\n        if erpConfig.AccessType == Database then:\n            queryBuilder = DatabaseQueryBuilder()\n                .ForERP(erpType)\n                .WithConnectionString(erpConfig.ConnectionString)\n                .WithSchema(erpConfig.Schema)\n                .WithTable(dataType + \"_table\")\n                .WithColumns(\"id\", \"created_at\", \"data\")\n                \n            if useIncrementalExtract && erpConfig.SupportsCDC:\n                queryBuilder.WithWhere(erpConfig.WatermarkColumn, \"&gt;\", extractConfig.LastExtractTime)\n            else:\n                queryBuilder.WithWhere(\"is_processed\", false)\n                \n            query = queryBuilder\n                .WithOrderBy(\"created_at\")\n                .WithLimit(erpConfig.BatchSize)\n                .WithCommandTimeout(erpConfig.TimeoutSeconds)\n                .Build()\n            \n            Log \"Executing database query: \" + query.GenerateSql()\n            \n            // Use bulkhead isolation for database connection\n            using (bulkhead = BulkheadPolicy.Execute(erpType + \"-database\", () =&gt; {\n                extractedData = extractor.ExtractFromDatabase(query, extractConfig)\n                return extractedData\n            }))\n\n        /// &lt;summary&gt;Handle API extraction mode&lt;/summary&gt;\n        else:\n            authBuilder = AuthenticationBuilder()\n                .WithApiKey(credentials.ApiKey)\n                .WithClientId(credentials.ClientId)\n                .WithClientSecret(credentials.ClientSecret)\n                \n            if erpConfig.SupportsMutualTLS:\n                authBuilder.WithClientCertificate(credentials.ClientCertificate)\n                \n            auth = authBuilder.Build()\n            \n            requestBuilder = APIRequestBuilder()\n                .ForERP(erpType)\n                .WithEndpoint(erpConfig.BaseUrl + \"/api/v2/sales\")\n                .WithMethod(GET)\n                .WithAuthentication(auth)\n                .WithHeaders(erpConfig.RequiredHeaders)\n                \n            if useIncrementalExtract && erpConfig.SupportsCDC:\n                requestBuilder.WithQueryParameters({\n                    \"companyId\": erpConfig.CompanyId,\n                    \"warehouse\": erpConfig.WarehouseId,\n                    \"pageSize\": erpConfig.PageSize.toString(),\n                    \"changedSince\": extractConfig.LastExtractTime.toISOString()\n                })\n            else:\n                requestBuilder.WithQueryParameters({\n                    \"companyId\": erpConfig.CompanyId,\n                    \"warehouse\": erpConfig.WarehouseId,\n                    \"pageSize\": erpConfig.PageSize.toString()\n                })\n                \n            request = requestBuilder\n                .WithRetryPolicy(erpConfig.MaxRetries)\n                .WithTimeout(erpConfig.TimeoutSeconds)\n                .Build()\n            \n            Log \"Executing API request to \" + erpConfig.BaseUrl + \"/api/v2/sales\"\n            \n            // Execute with circuit breaker protection\n            extractedData = CircuitBreakerPolicy\n                .ForService(\"erp-api-\" + erpType)\n                .Execute(() =&gt; extractor.Extract(request, extractConfig))\n\n        /// &lt;summary&gt;Process extracted data as before&lt;/summary&gt;\n        // Validate data against contract\n        validationResult = validator.Validate(extractedData)\n        \n        // Transform the data\n        transformedData = transformer.Transform(extractedData)\n        \n        // Upload to S3\n        uploadResult = uploader.Upload(transformedData, uploadConfig)\n        \n        Log \"Extraction ingestion process completed successfully\"\n    }",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion (Version 3)"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design-v3.html#conclusion",
    "href": "archive/erp-extractor-design-v3.html#conclusion",
    "title": "Design Document for Distributor Data Extraction Ingestion (Version 3)",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nThis design document outlines an optimized approach to the distributor data extraction pipeline. By leveraging .NET’s built-in dependency injection capabilities more effectively, we’ve eliminated the need for custom registry classes while maintaining the flexibility to dynamically resolve the right components based on runtime parameters.\nThe simplified architecture uses factory functions registered directly in the DI container to dynamically select the appropriate extractors, transformers, and uploaders based on the ERP type and data type. This approach:\n\nReduces complexity by eliminating custom registry abstractions\nImproves maintainability by centralizing resolution logic in the DI container\nEnhances testability by providing clear injection patterns\nPreserves all the flexibility of the previous design\nLeverages native .NET capabilities rather than custom implementations\n\nThe system maintains its comprehensive resilience features, security controls, and data quality validations while implementing a more elegant approach to component resolution. This design is more aligned with .NET best practices and will be easier to maintain and extend as new ERP integrations are added.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion (Version 3)"
    ]
  },
  {
    "objectID": "archive/dpp-tasking-outline.html",
    "href": "archive/dpp-tasking-outline.html",
    "title": "DPP Project Tasking Outline",
    "section": "",
    "text": "This document outlines the implementation tasks for the enhanced Distributor Data Extraction Ingestion project. Each task is designed to be completed within 1-3 days and includes clear objectives, completion criteria, and time estimates.",
    "crumbs": [
      "Archive",
      "DPP Project Tasking Outline"
    ]
  },
  {
    "objectID": "archive/dpp-tasking-outline.html#project-overview",
    "href": "archive/dpp-tasking-outline.html#project-overview",
    "title": "DPP Project Tasking Outline",
    "section": "",
    "text": "This document outlines the implementation tasks for the enhanced Distributor Data Extraction Ingestion project. Each task is designed to be completed within 1-3 days and includes clear objectives, completion criteria, and time estimates.",
    "crumbs": [
      "Archive",
      "DPP Project Tasking Outline"
    ]
  },
  {
    "objectID": "archive/dpp-tasking-outline.html#phase-1-foundation-and-core-infrastructure-weeks-1-2",
    "href": "archive/dpp-tasking-outline.html#phase-1-foundation-and-core-infrastructure-weeks-1-2",
    "title": "DPP Project Tasking Outline",
    "section": "Phase 1: Foundation and Core Infrastructure (Weeks 1-2)",
    "text": "Phase 1: Foundation and Core Infrastructure (Weeks 1-2)\n\nTask 1.1: Set Up Project Structure and Development Environment\nObjective: Create the initial project structure and configure the development environment.\nCompletion Criteria:\n\nRepository initialized with proper structure and README\nSolution structure created with defined projects and namespaces\nDocker Compose configuration for local development with HashiCorp Vault, FerretDB, and minio (S3 alternative)\nCI/CD pipeline templates defined\n\nEstimated Time: 2 days\n\n\nTask 1.2: Implement Basic Dependency Injection Framework\nObjective: Set up the DI container and register core services.\nCompletion Criteria:\n\nProgram.cs and CreateHostBuilder implemented\nCore service interfaces defined\nBasic DI container configuration with service registration\n\nEstimated Time: 1 day\n\n\nTask 1.3: Implement HashiCorp Vault Integration\nObjective: Create the Vault service for credential management.\nCompletion Criteria:\n\nVaultService class implemented with basic operations\nAuthentication methods supported (AppRole, Token)\nUnit tests for Vault interaction\nConfiguration model for Vault settings\n\nEstimated Time: 2 days\n\n\nTask 1.4: Implement FerretDB Integration\nObjective: Create the FerretDB service for configuration management.\nCompletion Criteria:\n\nFerretDBService class implemented with MongoDB driver\nBasic CRUD operations for configuration management\nUnit tests for FerretDB interaction\nConfiguration model for FerretDB settings\n\nEstimated Time: 2 days\n\n\nTask 1.5: Implement AWS S3 Integration\nObjective: Set up the S3 client for data upload.\nCompletion Criteria:\n\nS3DataUploader class implemented\nUpload functionality with metadata support\nUnit tests for S3 interaction\nConfiguration model for S3 settings\n\nEstimated Time: 1 day\n\n\nTask 1.6: Implement Core Models and Interfaces\nObjective: Define the core domain models and interfaces for the system.\nCompletion Criteria:\n\nERPConfiguration model implemented\nERPCredentials model implemented\nUploadConfiguration model implemented\nCore interfaces (IExtractor, ITransformer, IUploader) defined\n\nEstimated Time: 1 day",
    "crumbs": [
      "Archive",
      "DPP Project Tasking Outline"
    ]
  },
  {
    "objectID": "archive/dpp-tasking-outline.html#phase-2-core-functionality-implementation-weeks-3-4",
    "href": "archive/dpp-tasking-outline.html#phase-2-core-functionality-implementation-weeks-3-4",
    "title": "DPP Project Tasking Outline",
    "section": "Phase 2: Core Functionality Implementation (Weeks 3-4)",
    "text": "Phase 2: Core Functionality Implementation (Weeks 3-4)\n\nTask 2.1: Implement Registry Pattern Components\nObjective: Create the registry classes for dynamic component resolution.\nCompletion Criteria:\n\nERPRegistry implemented\nExtractorRegistry implemented\nTransformationRegistry implemented\nUploaderRegistry implemented\nUnit tests for registry functionality\n\nEstimated Time: 2 days\n\n\nTask 2.2: Implement API Mode Extraction Components\nObjective: Create the components for API-based extraction.\nCompletion Criteria:\n\nAPIRequestBuilder implemented with fluent interface\nAuthenticationBuilder implemented\nAPI-based extractor implementation\nUnit tests for API extraction\n\nEstimated Time: 3 days\n\n\nTask 2.3: Implement Database Mode Extraction Components\nObjective: Create the components for database-based extraction.\nCompletion Criteria:\n\nDatabaseQueryBuilder implemented with fluent interface\nDatabaseQuery class with SQL generation\nDatabase-based extractor implementation\nUnit tests for database extraction\n\nEstimated Time: 3 days\n\n\nTask 2.4: Implement Basic Transformation Logic\nObjective: Create the transformation components for column standardization.\nCompletion Criteria:\n\nBasic transformer implementation\nColumn mapping functionality\nParquet conversion logic\nUnit tests for transformation\n\nEstimated Time: 2 days\n\n\nTask 2.5: Implement ERPService Core Logic\nObjective: Create the main orchestration logic for the extraction ingestion process.\nCompletion Criteria:\n\nProcessERPData method implemented with basic flow\nIntegration with registries and components\nSupport for both API and Database modes\nError handling for basic scenarios\n\nEstimated Time: 3 days\n\n\nTask 2.6: Implement Command-Line Interface\nObjective: Create the command-line interface for the application.\nCompletion Criteria:\n\nSystem.CommandLine integration\nCommand-line argument parsing\nHelp documentation\nExit code handling\n\nEstimated Time: 1 day",
    "crumbs": [
      "Archive",
      "DPP Project Tasking Outline"
    ]
  },
  {
    "objectID": "archive/dpp-tasking-outline.html#phase-3-resilience-and-error-handling-weeks-5-6",
    "href": "archive/dpp-tasking-outline.html#phase-3-resilience-and-error-handling-weeks-5-6",
    "title": "DPP Project Tasking Outline",
    "section": "Phase 3: Resilience and Error Handling (Weeks 5-6)",
    "text": "Phase 3: Resilience and Error Handling (Weeks 5-6)\n\nTask 3.1: Implement Circuit Breaker Pattern\nObjective: Add circuit breaker protection for external dependencies.\nCompletion Criteria:\n\nCircuit breaker implementation for Vault\nCircuit breaker implementation for FerretDB\nCircuit breaker implementation for S3\nCircuit breaker implementation for API calls\nUnit tests for circuit breaker functionality\n\nEstimated Time: 2 days\n\n\nTask 3.2: Implement Bulkhead Pattern\nObjective: Add resource isolation using bulkheads.\nCompletion Criteria:\n\nConnection pool isolation for different ERP types\nResource allocation for critical vs. non-critical operations\nBulkhead configuration model\nUnit tests for bulkhead functionality\n\nEstimated Time: 2 days\n\n\nTask 3.3: Implement Retry Strategies with Exponential Backoff\nObjective: Enhance retry logic with exponential backoff and jitter.\nCompletion Criteria:\n\nRetryPolicyBuilder implemented\nExponential backoff strategy with jitter\nIntegration with HTTP client factory\nUnit tests for retry policies\n\nEstimated Time: 1 day\n\n\nTask 3.4: Implement Self-Healing Procedures\nObjective: Add automatic recovery procedures for common failure scenarios.\nCompletion Criteria:\n\nToken renewal for expired credentials\nConnection pool refresh for stale connections\nAutomatic cleanup for temporary resources\nUnit tests for self-healing functionality\n\nEstimated Time: 3 days\n\n\nTask 3.5: Enhance Error Handling and Logging\nObjective: Improve error handling with classification and structured logging.\nCompletion Criteria:\n\nError classification (transient vs. persistent)\nStructured logging with correlation IDs\nContext-enriched log entries\nIntegration with Serilog\n\nEstimated Time: 2 days\n\n\nTask 3.6: Implement Health Checks\nObjective: Create health check endpoints for system monitoring.\nCompletion Criteria:\n\nHealth check implementation for Vault\nHealth check implementation for FerretDB\nHealth check implementation for S3\nHealth check API endpoint\nIntegration with monitoring system\n\nEstimated Time: 1 day",
    "crumbs": [
      "Archive",
      "DPP Project Tasking Outline"
    ]
  },
  {
    "objectID": "archive/dpp-tasking-outline.html#phase-4-security-enhancements-weeks-7-8",
    "href": "archive/dpp-tasking-outline.html#phase-4-security-enhancements-weeks-7-8",
    "title": "DPP Project Tasking Outline",
    "section": "Phase 4: Security Enhancements (Weeks 7-8)",
    "text": "Phase 4: Security Enhancements (Weeks 7-8)\n\nTask 4.1: Implement Credential Caching\nObjective: Add caching to credential retrieval for improved performance.\nCompletion Criteria:\n\nCachedCredentialProviderDecorator implemented\nTTL-based cache invalidation\nThread-safe caching mechanism\nUnit tests for caching functionality\n\nEstimated Time: 1 day\n\n\nTask 4.2: Implement Least Privilege Access\nObjective: Enhance security with dynamic, operation-specific credentials.\nCompletion Criteria:\n\nLeastPrivilegeCredentialProvider implemented\nOperation-specific credential generation\nCredential scoping based on context\nIntegration with Vault dynamic secrets\n\nEstimated Time: 2 days\n\n\nTask 4.3: Implement Field-Level Encryption\nObjective: Add encryption for sensitive fields.\nCompletion Criteria:\n\nEncryptionService implemented\nField-level encryption/decryption\nKey management integration\nUnit tests for encryption functionality\n\nEstimated Time: 3 days\n\n\nTask 4.4: Implement Data Masking\nObjective: Create data masking functionality for non-production environments.\nCompletion Criteria:\n\nDataMaskingService implemented\nVarious masking techniques (hashing, tokenization)\nConfiguration model for masking rules\nUnit tests for masking functionality\n\nEstimated Time: 2 days\n\n\nTask 4.5: Implement Mutual TLS\nObjective: Enhance secure communication with mutual TLS.\nCompletion Criteria:\n\nmTLS configuration for HTTP clients\nCertificate management integration\nmTLS support in API request builder\nUnit tests for mTLS functionality\n\nEstimated Time: 2 days\n\n\nTask 4.6: Implement Audit Logging\nObjective: Add comprehensive audit logging for security compliance.\nCompletion Criteria:\n\nAuditLogger implemented\nIntegration with Vault audit backend\nOperational audit events defined\nCompliance reporting capabilities\n\nEstimated Time: 1 day",
    "crumbs": [
      "Archive",
      "DPP Project Tasking Outline"
    ]
  },
  {
    "objectID": "archive/dpp-tasking-outline.html#phase-5-data-quality-and-governance-weeks-9-10",
    "href": "archive/dpp-tasking-outline.html#phase-5-data-quality-and-governance-weeks-9-10",
    "title": "DPP Project Tasking Outline",
    "section": "Phase 5: Data Quality and Governance (Weeks 9-10)",
    "text": "Phase 5: Data Quality and Governance (Weeks 9-10)\n\nTask 5.1: Implement Data Contract Validator\nObjective: Create validation logic for data contracts.\nCompletion Criteria:\n\nDataContractValidator implemented\nSchema validation functionality\nValue validation against business rules\nValidation result model with severity levels\nUnit tests for validation functionality\n\nEstimated Time: 3 days\n\n\nTask 5.2: Implement Data Lineage Tracking\nObjective: Add data lineage capabilities for audit and compliance.\nCompletion Criteria:\n\nDataLineageService implemented\nLineage record creation and management\nTransformation tracking\nIntegration with metadata services\nUnit tests for lineage functionality\n\nEstimated Time: 2 days\n\n\nTask 5.3: Implement Data Catalog Integration\nObjective: Create integration with data catalog for metadata management.\nCompletion Criteria:\n\nCatalogService implemented\nDataset metadata management\nSchema versioning support\nUnit tests for catalog functionality\n\nEstimated Time: 2 days\n\n\nTask 5.4: Implement Schema Evolution Support\nObjective: Add capabilities for handling schema changes.\nCompletion Criteria:\n\nSchemaVersionManager implemented\nBackward compatibility handling\nSchema migration support\nUnit tests for schema evolution\n\nEstimated Time: 3 days\n\n\nTask 5.5: Implement Data Quality Metrics Collection\nObjective: Add collection of data quality metrics.\nCompletion Criteria:\n\nDataQualityMetricsCollector implemented\nQuality dimension measurements\nIntegration with metrics service\nUnit tests for metrics collection\n\nEstimated Time: 2 days\n\n\nTask 5.6: Implement Data Quality Reporting\nObjective: Create reporting capabilities for data quality.\nCompletion Criteria:\n\nDataQualityReportGenerator implemented\nQuality issue summarization\nTrend analysis for quality metrics\nIntegration with notification system\n\nEstimated Time: 2 days",
    "crumbs": [
      "Archive",
      "DPP Project Tasking Outline"
    ]
  },
  {
    "objectID": "archive/dpp-tasking-outline.html#phase-6-performance-optimization-weeks-11-12",
    "href": "archive/dpp-tasking-outline.html#phase-6-performance-optimization-weeks-11-12",
    "title": "DPP Project Tasking Outline",
    "section": "Phase 6: Performance Optimization (Weeks 11-12)",
    "text": "Phase 6: Performance Optimization (Weeks 11-12)\n\nTask 6.1: Implement Incremental Extraction\nObjective: Add support for incremental data extraction.\nCompletion Criteria:\n\nExtractConfigBuilder implemented\nChange Data Capture (CDC) support\nWatermark management\nUnit tests for incremental extraction\n\nEstimated Time: 3 days\n\n\nTask 6.2: Implement Batch Processing\nObjective: Enhance performance with batch processing capabilities.\nCompletion Criteria:\n\nBatchProcessor implemented\nMemory-efficient processing\nProgress tracking\nUnit tests for batch processing\n\nEstimated Time: 2 days\n\n\nTask 6.3: Implement Data Compression\nObjective: Add compression support for improved efficiency.\nCompletion Criteria:\n\nCompressionService implemented\nMultiple compression algorithm support\nCompression level configuration\nUnit tests for compression functionality\n\nEstimated Time: 1 day\n\n\nTask 6.4: Implement Metrics Collection\nObjective: Add comprehensive metrics collection for performance monitoring.\nCompletion Criteria:\n\nMetricsService implemented\nIntegration with Prometheus\nCustom dimensions and labels\nUnit tests for metrics functionality\n\nEstimated Time: 2 days\n\n\nTask 6.5: Implement Performance Benchmarking\nObjective: Create benchmarking capabilities for performance testing.\nCompletion Criteria:\n\nBenchmarkRunner implemented\nStandard performance scenarios\nResult comparison and reporting\nIntegration with CI/CD pipeline\n\nEstimated Time: 2 days\n\n\nTask 6.6: Optimize Resource Usage\nObjective: Tune system for optimal resource utilization.\nCompletion Criteria:\n\nMemory usage optimization\nThread pool configuration\nConnection pool tuning\nPerformance test results showing improvement\n\nEstimated Time: 3 days",
    "crumbs": [
      "Archive",
      "DPP Project Tasking Outline"
    ]
  },
  {
    "objectID": "archive/dpp-tasking-outline.html#phase-7-operational-excellence-weeks-13-14",
    "href": "archive/dpp-tasking-outline.html#phase-7-operational-excellence-weeks-13-14",
    "title": "DPP Project Tasking Outline",
    "section": "Phase 7: Operational Excellence (Weeks 13-14)",
    "text": "Phase 7: Operational Excellence (Weeks 13-14)\n\nTask 7.1: Implement Feature Flag Management\nObjective: Add feature flag capabilities for gradual rollout.\nCompletion Criteria:\n\nFeatureFlagService implemented\nFlag configuration management\nContext-based flag evaluation\nUnit tests for feature flags\n\nEstimated Time: 2 days\n\n\nTask 7.2: Implement Distributed Tracing\nObjective: Add distributed tracing for end-to-end visibility.\nCompletion Criteria:\n\nTracingService implemented\nIntegration with OpenTelemetry\nTrace correlation across components\nTrace sampling configuration\n\nEstimated Time: 2 days\n\n\nTask 7.3: Implement OpenAPI Documentation\nObjective: Create API documentation for service interfaces.\nCompletion Criteria:\n\nOpenAPI configuration\nAPI endpoint documentation\nModel documentation\nSwagger UI integration\n\nEstimated Time: 1 day\n\n\nTask 7.4: Create Operational Runbooks\nObjective: Develop runbooks for common operational procedures.\nCompletion Criteria:\n\nInstallation and deployment guide\nTroubleshooting procedures\nMonitoring guidelines\nDisaster recovery procedures\n\nEstimated Time: 3 days\n\n\nTask 7.5: Implement GitOps Configuration\nObjective: Set up GitOps-based configuration management.\nCompletion Criteria:\n\nInfrastructure as Code templates\nConfiguration as Code approach\nDeployment pipeline integration\nChange approval workflow\n\nEstimated Time: 2 days\n\n\nTask 7.6: Implement Blue-Green Deployment Support\nObjective: Add support for zero-downtime deployments.\nCompletion Criteria:\n\nDeployment strategy implementation\nVersion compatibility verification\nRollback procedures\nLoad balancer integration\n\nEstimated Time: 3 days",
    "crumbs": [
      "Archive",
      "DPP Project Tasking Outline"
    ]
  },
  {
    "objectID": "archive/dpp-tasking-outline.html#phase-8-integration-and-testing-weeks-15-16",
    "href": "archive/dpp-tasking-outline.html#phase-8-integration-and-testing-weeks-15-16",
    "title": "DPP Project Tasking Outline",
    "section": "Phase 8: Integration and Testing (Weeks 15-16)",
    "text": "Phase 8: Integration and Testing (Weeks 15-16)\n\nTask 8.1: Implement Integration Test Harness\nObjective: Create test infrastructure for comprehensive integration testing.\nCompletion Criteria:\n\nTest harness framework\nMock ERP implementations\nTest data generation\nTest execution automation\n\nEstimated Time: 3 days\n\n\nTask 8.2: Implement End-to-End Tests\nObjective: Create end-to-end tests for critical workflows.\nCompletion Criteria:\n\nE2E test scenarios for API mode\nE2E test scenarios for Database mode\nTest assertions and verification\nTest reporting\n\nEstimated Time: 3 days\n\n\nTask 8.3: Implement Performance Tests\nObjective: Create performance tests for system evaluation.\nCompletion Criteria:\n\nLoad test scenarios\nStress test scenarios\nPerformance metrics collection\nTest result analysis\n\nEstimated Time: 2 days\n\n\nTask 8.4: Implement Security Tests\nObjective: Create security tests to validate protection measures.\nCompletion Criteria:\n\nAuthentication and authorization tests\nEncryption verification\nSecure communication tests\nAudit log verification\n\nEstimated Time: 2 days\n\n\nTask 8.5: Implement Fault Injection Tests\nObjective: Create tests to validate resilience capabilities.\nCompletion Criteria:\n\nDependency failure simulations\nNetwork degradation tests\nResource exhaustion tests\nRecovery verification\n\nEstimated Time: 2 days\n\n\nTask 8.6: Conduct System Integration Testing\nObjective: Perform comprehensive integration testing with all components.\nCompletion Criteria:\n\nAll subsystems tested together\nEdge cases and boundary conditions tested\nPerformance under load verified\nDocumentation of test results\n\nEstimated Time: 3 days",
    "crumbs": [
      "Archive",
      "DPP Project Tasking Outline"
    ]
  },
  {
    "objectID": "archive/dpp-tasking-outline.html#phase-9-finalization-and-deployment-weeks-17-18",
    "href": "archive/dpp-tasking-outline.html#phase-9-finalization-and-deployment-weeks-17-18",
    "title": "DPP Project Tasking Outline",
    "section": "Phase 9: Finalization and Deployment (Weeks 17-18)",
    "text": "Phase 9: Finalization and Deployment (Weeks 17-18)\n\nTask 9.1: Finalize Documentation\nObjective: Complete all system documentation.\nCompletion Criteria:\n\nArchitecture documentation finalized\nAPI documentation completed\nDevelopment guide updated\nOperational procedures documented\n\nEstimated Time: 3 days\n\n\nTask 9.2: Conduct Code Review and Cleanup\nObjective: Perform comprehensive code review and cleanup.\nCompletion Criteria:\n\nCode review completed\nCode quality issues addressed\nTechnical debt remediated\nCode documentation updated\n\nEstimated Time: 2 days\n\n\nTask 9.3: Prepare Release Artifacts\nObjective: Create release artifacts for deployment.\nCompletion Criteria:\n\nRelease notes prepared\nVersioned artifacts created\nDeployment packages assembled\nRelease verification checklist completed\n\nEstimated Time: 1 day\n\n\nTask 9.4: Set Up Monitoring and Alerting\nObjective: Configure production monitoring and alerting.\nCompletion Criteria:\n\nMetrics dashboards created\nAlert rules configured\nOn-call procedures defined\nMonitoring documentation completed\n\nEstimated Time: 2 days\n\n\nTask 9.5: Conduct Deployment Rehearsal\nObjective: Perform a rehearsal of the production deployment.\nCompletion Criteria:\n\nStaging environment deployment completed\nVerification procedures executed\nRollback procedures tested\nDeployment timing measured\n\nEstimated Time: 1 day\n\n\nTask 9.6: Execute Production Deployment\nObjective: Deploy the system to production.\nCompletion Criteria:\n\nProduction deployment completed\nPost-deployment verification performed\nStakeholder sign-off obtained\nTransition to operational support\n\nEstimated Time: 1 day",
    "crumbs": [
      "Archive",
      "DPP Project Tasking Outline"
    ]
  },
  {
    "objectID": "archive/dpp-tasking-outline.html#project-timeline-summary",
    "href": "archive/dpp-tasking-outline.html#project-timeline-summary",
    "title": "DPP Project Tasking Outline",
    "section": "Project Timeline Summary",
    "text": "Project Timeline Summary\n\nPhase 1: Foundation and Core Infrastructure - Weeks 1-2\nPhase 2: Core Functionality Implementation - Weeks 3-4\nPhase 3: Resilience and Error Handling - Weeks 5-6\nPhase 4: Security Enhancements - Weeks 7-8\nPhase 5: Data Quality and Governance - Weeks 9-10\nPhase 6: Performance Optimization - Weeks 11-12\nPhase 7: Operational Excellence - Weeks 13-14\nPhase 8: Integration and Testing - Weeks 15-16\nPhase 9: Finalization and Deployment - Weeks 17-18\n\nTotal estimated project duration: 18 weeks",
    "crumbs": [
      "Archive",
      "DPP Project Tasking Outline"
    ]
  },
  {
    "objectID": "archive/upsert-ops-cost-analysis.html",
    "href": "archive/upsert-ops-cost-analysis.html",
    "title": "UPSERT Operations Cost Analysis: AWS Glue vs EMR",
    "section": "",
    "text": "This report compares the costs and trade-offs between AWS Glue and Amazon EMR for performing upsert operations. An upsert (update-insert) operation updates existing records if they exist or inserts new records if they don’t, commonly used in data warehousing and ETL processes. We analyze historical usage patterns to project annual costs and evaluate the management overhead of each service. The analysis includes instance type recommendations, pricing models, and technical specifications to help you make an informed decision for your use case.\n\n\nCode\nimport polars as pl\nfrom datetime import datetime\nimport plotly.express as px\n\n# AWS Pricing Constants (as of January 2025)\nAWS_PRICING = {\n    'glue': {\n        'dpu_hour_cost': 0.44,      # Cost per DPU-hour for AWS Glue\n        'dpu_per_gb': 2,            # DPUs required per GB of data (based on AWS recommendations)\n        'processing_factor': 1.5     # Overhead factor for UPSERT operations vs regular processing\n    },\n    'emr_m6g_xlarge': {\n        'ec2_hour_cost': 0.154,     # On-demand hourly rate for m6g.xlarge instance\n        'emr_hour_cost': 0.039,     # EMR service hourly rate for m6g.xlarge\n        'processing_factor': 1.2,    # Processing efficiency factor based on benchmark testing\n        'specs': {\n            'vcpu': 4,\n            'memory': '16 GiB',\n            'storage': 'EBS Only',\n            'network': 'Up to 10 Gigabit'\n        }\n    }\n}\n\ndef estimate_glue_cost(data_size_gb):\n    \"\"\"\n    Estimates AWS Glue processing cost and time for given data size\n\n    Args:\n        data_size_gb (float): Size of data to process in gigabytes\n\n    Returns:\n        dict: Contains estimated cost and processing time\n\n    Note: Includes 1.5x overhead factor for UPSERT operations\n    \"\"\"\n    try:\n        pricing = AWS_PRICING['glue']\n        processing_hours = (data_size_gb * pricing['processing_factor']) / pricing['dpu_per_gb']\n        single_run_cost = processing_hours * pricing['dpu_hour_cost']\n        return {\n            \"single_run_cost\": round(single_run_cost, 2),\n            \"processing_hours\": round(processing_hours, 2)\n        }\n    except Exception as e:\n        print(f\"Error calculating Glue cost: {str(e)}\")\n        return None\n\ndef estimate_emr_cost(data_size_gb):\n    \"\"\"\n    Estimates EMR (m6g.xlarge) processing cost and time for given data size\n\n    Args:\n        data_size_gb (float): Size of data to process in gigabytes\n\n    Returns:\n        dict: Contains estimated cost and processing time\n\n    Note: Includes 1.2x overhead factor based on benchmark testing\n    \"\"\"\n    try:\n        pricing = AWS_PRICING['emr_m6g_xlarge']\n        processing_hours = data_size_gb * pricing['processing_factor']\n        hourly_rate = pricing['ec2_hour_cost'] + pricing['emr_hour_cost']\n        total_cost = processing_hours * hourly_rate\n        return {\n            \"single_run_cost\": round(total_cost, 2),\n            \"processing_hours\": round(processing_hours, 2)\n        }\n    except Exception as e:\n        print(f\"Error calculating EMR cost: {str(e)}\")\n        return None\n\n\n\n\n\n\nCode\n# Load ETL history from parquet\ndf = pl.read_parquet('./scripts/etl_history_2024.parquet')\n\n# Group by date and calculate daily statistics\ndaily_stats = (\n    df.with_columns(pl.col(\"timestamp\").dt.date().alias(\"date\"))\n    .group_by(\"date\")\n    .agg([\n        pl.col(\"rows\").sum().alias(\"total_rows\"),\n        pl.len().alias(\"number_of_loads\")\n    ])\n    .sort(\"date\")\n)\n\n# Calculate data size (1 KB per row based on average record size in production)\ndaily_stats = daily_stats.with_columns(\n    (pl.col(\"total_rows\") / (1024 * 1024)).alias(\"data_size_gb\")\n)\n\n# Calculate costs\nGLUE_FACTOR = 0.33  # data_size_gb * 0.75 * 0.44 = data_size_gb * 0.33\nEMR_FACTOR = 0.2316 # data_size_gb * 1.2 * (0.154 + 0.039) = data_size_gb * 0.2316\n\ndaily_stats = daily_stats.with_columns([\n    (pl.col(\"data_size_gb\") * GLUE_FACTOR).round(2).alias(\"glue_daily_cost\"),\n    (pl.col(\"data_size_gb\") * EMR_FACTOR).round(2).alias(\"emr_daily_cost\")\n])\n\n# print(\"ETL Load Statistics Summary\")\n# print(\"-\" * 50)\n# print(f\"| Metric                          | Value              |\")\n# print(f\"|-------------------------------- |-------------------|\")\n# print(f\"| Date Range                      | {daily_stats['date'].min()} to {daily_stats['date'].max()} |\")\n# print(f\"| Average Daily Rows Processed    | {daily_stats['total_rows'].mean():,.0f} rows/day |\")\n# print(f\"| Average Daily Load Operations   | {daily_stats['number_of_loads'].mean():.1f} loads/day |\")\n# print(f\"| Average Data Size              | {daily_stats['data_size_gb'].mean():.2f} GB/day |\")\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nDate Range\n2024-01-01 to 2024-12-30\n\n\nAverage Daily Rows Processed\n105,218 rows/day\n\n\nAverage Daily Load Operations\n59.2 loads/day\n\n\nAverage Data Size\n0.10 GB/day\n\n\n\n\n\n\n\n\nCode\n# Generate date range as an eager DataFrame\ndate_range_df = pl.select(\n    pl.date_range(\n        datetime(2024, 1, 1),\n        datetime(2024, 12, 31),\n        \"1d\"\n    ).alias(\"date\")\n)\n\n# Join once\ndaily_stats_filled = date_range_df.join(\n    daily_stats,\n    on='date',\n    how='left'\n).fill_null(0)\n\n# Calculate yearly projections with confidence intervals\nglue_yearly = daily_stats['glue_daily_cost'].mean() * 365\nglue_std = daily_stats['glue_daily_cost'].std() * (365 ** 0.5)\nemr_yearly = daily_stats['emr_daily_cost'].mean() * 365\nemr_std = daily_stats['emr_daily_cost'].std() * (365 ** 0.5)\n\nprint(\"\\nProjected Yearly Costs (Including Data Transfer Fees)\")\nprint(\"-\" * 70)\nprint(f\"AWS Glue: ${glue_yearly:,.2f} ± ${glue_std:,.2f}\")\nprint(f\"EMR:      ${emr_yearly:,.2f} ± ${emr_std:,.2f}\")\nprint(\"\\nNote: Confidence intervals based on historical variance\")\n\n# Prepare visualization data\ncost_comparison = daily_stats_filled.select([\n    'date',\n    pl.col('glue_daily_cost').alias('AWS Glue'),\n    pl.col('emr_daily_cost').alias('EMR')\n])\n\nfig = px.line(\n    cost_comparison.to_pandas(),\n    x='date',\n    y=['AWS Glue', 'EMR'],\n    title='Daily Cost Comparison: AWS Glue vs EMR',\n    labels={'value': 'Cost ($)', 'variable': 'Service'}\n)\n\nfig.update_layout(\n    xaxis_range=['2024-01-01', '2024-12-31'],\n    yaxis_title=\"Cost ($)\",\n    legend_title=\"Service\"\n)\n\nfig.show()\n\n\n\nProjected Yearly Costs (Including Data Transfer Fees)\n----------------------------------------------------------------------\nAWS Glue: $11.87 ± $0.70\nEMR:      $8.43 ± $0.49\n\nNote: Confidence intervals based on historical variance\n\n\n                                                \n\n\n\n\n\nFor our workload (100 upserts/day, ~1GB total), here’s the optimal EMR configuration:\nCore Node Configuration:\n  Instance:\n    Type: m6g.xlarge (ARM-based)\n    Specifications:\n      vCPU: 4\n      Memory: 16 GiB\n      Storage: EBS Only\n      Network: Up to 10 Gigabit\n    Pricing:\n      On-Demand: $0.1075/hour\n      Spot: $0.0323/hour (70% savings)\n  Cluster Settings:\n    Count: 1-2 (Auto-scaling)\n    Spark Configuration:\n      executor.memory: 12G # 75% of instance memory for stability\n      executor.cores: 3 # Leaves 1 core for OS operations\n      spark.memory.fraction: 0.8\n\nMaster Node Configuration:\n  Instance: m6g.large\n  Count: 1 (On-Demand recommended for stability)\n\n\n\n\n\nSpecification\nDetails\n\n\n\n\nEC2 On-Demand Rate\n$0.154/hour\n\n\nEMR Rate\n$0.039/hour\n\n\nvCPU\n4\n\n\nMemory\n16 GiB\n\n\nStorage\nEBS Only\n\n\nNetwork Performance\nUp to 10 Gigabit\n\n\nTotal Hourly Cost\n$0.193/hour ($0.154 + $0.039)\n\n\n\n\n\n\n\nOptimal memory-to-CPU ratio for Spark workloads\nARM instances provide 20% cost savings compared to x86\nSufficient capacity for concurrent small jobs\nProven performance for MSSQL/Iceberg operations\n\n\n\n\n\nm6a.xlarge: Recommended when x86 compatibility is required\nr6g.large: Suitable for memory-intensive UPSERT operations\nt3.xlarge: Cost-effective for sporadic workloads with burstable performance\n\n\n\n\nEMR provides multiple options for adjusting instance types as requirements evolve:\n\nFor Running Clusters\n\nUse Instance Fleets to mix different instance types\nGradually scale down old groups while scaling up new ones\nLeverage EMR’s automatic instance selection\n\nFor New Clusters\n\nSpecify new instance types in configuration\nUse temporary clusters for performance testing\nNo long-term commitment to specific instance types\n\n\n\n\n\n\n\n\n\nAWS Glue\n\nPer-second billing at $0.44/DPU-Hour\n1-minute minimum billing increment\nAutomatic scaling based on workload\nNo infrastructure management costs\n\nEMR\n\nBilling based on cluster uptime\nSignificant savings available with Spot instances\nManual scaling control\nAdditional costs for cluster management\n\n\n\n\n\n\nAWS Glue\n\nZero infrastructure management\nNative integration with AWS CloudWatch\nLimited configuration options\nAutomatic version upgrades\n\nEMR\n\nComplete cluster control\nCustomizable configurations\nHigher maintenance requirements\nManual version management\n\n\n\n\n\n\n\n\n\nAutomatic script versioning\nIntegration with AWS Backup\nMulti-region deployment support\n99.9% availability SLA\n\n\n\n\n\nManual backup procedures required\nCustom DR solutions needed\nMulti-AZ deployment options\nNo standard SLA\n\n\n\n\n\n\n\n\nNative AWS IAM integration\nBuilt-in encryption at rest\nVPC support\nAWS Shield protection\n\n\n\n\n\nCustomizable security groups\nManual encryption configuration\nVPC isolation required\nCustom security tools support\n\n\n\n\n\n\n\n\nExport job definitions\nConvert to Spark code\nSetup cluster management\nEstimated time: 2-4 weeks\n\n\n\n\n\nConvert Spark code\nSetup AWS Glue crawlers\nConfigure job properties\nEstimated time: 1-3 weeks\n\n\n\n\n\nBased on production workload testing:\n\n\n\nMetric\nAWS Glue\nEMR\n\n\n\n\nAvg. Processing Time\n45 mins\n38 mins\n\n\nCold Start Time\n2-3 mins\n8-10 mins\n\n\nMax Throughput\n100 GB/hr\n120 GB/hr\n\n\nConcurrent Jobs\n10\nUnlimited\n\n\n\n\n\n\nChoose based on your primary requirements:\n\n\n\nTeam prefers managed services\nWorkload is sporadic or unpredictable\nQuick setup is prioritized (1-2 days)\nAWS-native tooling is preferred\nLimited DevOps resources available\n\n\n\n\n\nCost optimization is critical\nWorkload is consistent and predictable\nFine-grained control is required\nTeam has Spark expertise\nCustom configurations needed\n\n\n\n\n\nMixed workload characteristics\nDifferent teams have varying expertise\nCost optimization varies by workload\nGradual migration is preferred\n\n\n\n\n\n\nAWS Glue Pricing (Accessed: January 2025)\nAmazon EMR Pricing (Accessed: January 2025)\nAWS Glue Best Practices\nEMR Benchmarking Guide\nAWS Big Data Blog: Glue vs EMR Comparison",
    "crumbs": [
      "Archive",
      "UPSERT Operations Cost Analysis"
    ]
  },
  {
    "objectID": "archive/upsert-ops-cost-analysis.html#historical-analysis",
    "href": "archive/upsert-ops-cost-analysis.html#historical-analysis",
    "title": "UPSERT Operations Cost Analysis: AWS Glue vs EMR",
    "section": "",
    "text": "Code\n# Load ETL history from parquet\ndf = pl.read_parquet('./scripts/etl_history_2024.parquet')\n\n# Group by date and calculate daily statistics\ndaily_stats = (\n    df.with_columns(pl.col(\"timestamp\").dt.date().alias(\"date\"))\n    .group_by(\"date\")\n    .agg([\n        pl.col(\"rows\").sum().alias(\"total_rows\"),\n        pl.len().alias(\"number_of_loads\")\n    ])\n    .sort(\"date\")\n)\n\n# Calculate data size (1 KB per row based on average record size in production)\ndaily_stats = daily_stats.with_columns(\n    (pl.col(\"total_rows\") / (1024 * 1024)).alias(\"data_size_gb\")\n)\n\n# Calculate costs\nGLUE_FACTOR = 0.33  # data_size_gb * 0.75 * 0.44 = data_size_gb * 0.33\nEMR_FACTOR = 0.2316 # data_size_gb * 1.2 * (0.154 + 0.039) = data_size_gb * 0.2316\n\ndaily_stats = daily_stats.with_columns([\n    (pl.col(\"data_size_gb\") * GLUE_FACTOR).round(2).alias(\"glue_daily_cost\"),\n    (pl.col(\"data_size_gb\") * EMR_FACTOR).round(2).alias(\"emr_daily_cost\")\n])\n\n# print(\"ETL Load Statistics Summary\")\n# print(\"-\" * 50)\n# print(f\"| Metric                          | Value              |\")\n# print(f\"|-------------------------------- |-------------------|\")\n# print(f\"| Date Range                      | {daily_stats['date'].min()} to {daily_stats['date'].max()} |\")\n# print(f\"| Average Daily Rows Processed    | {daily_stats['total_rows'].mean():,.0f} rows/day |\")\n# print(f\"| Average Daily Load Operations   | {daily_stats['number_of_loads'].mean():.1f} loads/day |\")\n# print(f\"| Average Data Size              | {daily_stats['data_size_gb'].mean():.2f} GB/day |\")\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nDate Range\n2024-01-01 to 2024-12-30\n\n\nAverage Daily Rows Processed\n105,218 rows/day\n\n\nAverage Daily Load Operations\n59.2 loads/day\n\n\nAverage Data Size\n0.10 GB/day",
    "crumbs": [
      "Archive",
      "UPSERT Operations Cost Analysis"
    ]
  },
  {
    "objectID": "archive/upsert-ops-cost-analysis.html#cost-comparison",
    "href": "archive/upsert-ops-cost-analysis.html#cost-comparison",
    "title": "UPSERT Operations Cost Analysis: AWS Glue vs EMR",
    "section": "",
    "text": "Code\n# Generate date range as an eager DataFrame\ndate_range_df = pl.select(\n    pl.date_range(\n        datetime(2024, 1, 1),\n        datetime(2024, 12, 31),\n        \"1d\"\n    ).alias(\"date\")\n)\n\n# Join once\ndaily_stats_filled = date_range_df.join(\n    daily_stats,\n    on='date',\n    how='left'\n).fill_null(0)\n\n# Calculate yearly projections with confidence intervals\nglue_yearly = daily_stats['glue_daily_cost'].mean() * 365\nglue_std = daily_stats['glue_daily_cost'].std() * (365 ** 0.5)\nemr_yearly = daily_stats['emr_daily_cost'].mean() * 365\nemr_std = daily_stats['emr_daily_cost'].std() * (365 ** 0.5)\n\nprint(\"\\nProjected Yearly Costs (Including Data Transfer Fees)\")\nprint(\"-\" * 70)\nprint(f\"AWS Glue: ${glue_yearly:,.2f} ± ${glue_std:,.2f}\")\nprint(f\"EMR:      ${emr_yearly:,.2f} ± ${emr_std:,.2f}\")\nprint(\"\\nNote: Confidence intervals based on historical variance\")\n\n# Prepare visualization data\ncost_comparison = daily_stats_filled.select([\n    'date',\n    pl.col('glue_daily_cost').alias('AWS Glue'),\n    pl.col('emr_daily_cost').alias('EMR')\n])\n\nfig = px.line(\n    cost_comparison.to_pandas(),\n    x='date',\n    y=['AWS Glue', 'EMR'],\n    title='Daily Cost Comparison: AWS Glue vs EMR',\n    labels={'value': 'Cost ($)', 'variable': 'Service'}\n)\n\nfig.update_layout(\n    xaxis_range=['2024-01-01', '2024-12-31'],\n    yaxis_title=\"Cost ($)\",\n    legend_title=\"Service\"\n)\n\nfig.show()\n\n\n\nProjected Yearly Costs (Including Data Transfer Fees)\n----------------------------------------------------------------------\nAWS Glue: $11.87 ± $0.70\nEMR:      $8.43 ± $0.49\n\nNote: Confidence intervals based on historical variance",
    "crumbs": [
      "Archive",
      "UPSERT Operations Cost Analysis"
    ]
  },
  {
    "objectID": "archive/upsert-ops-cost-analysis.html#emr-instance-selection",
    "href": "archive/upsert-ops-cost-analysis.html#emr-instance-selection",
    "title": "UPSERT Operations Cost Analysis: AWS Glue vs EMR",
    "section": "",
    "text": "For our workload (100 upserts/day, ~1GB total), here’s the optimal EMR configuration:\nCore Node Configuration:\n  Instance:\n    Type: m6g.xlarge (ARM-based)\n    Specifications:\n      vCPU: 4\n      Memory: 16 GiB\n      Storage: EBS Only\n      Network: Up to 10 Gigabit\n    Pricing:\n      On-Demand: $0.1075/hour\n      Spot: $0.0323/hour (70% savings)\n  Cluster Settings:\n    Count: 1-2 (Auto-scaling)\n    Spark Configuration:\n      executor.memory: 12G # 75% of instance memory for stability\n      executor.cores: 3 # Leaves 1 core for OS operations\n      spark.memory.fraction: 0.8\n\nMaster Node Configuration:\n  Instance: m6g.large\n  Count: 1 (On-Demand recommended for stability)\n\n\n\n\n\nSpecification\nDetails\n\n\n\n\nEC2 On-Demand Rate\n$0.154/hour\n\n\nEMR Rate\n$0.039/hour\n\n\nvCPU\n4\n\n\nMemory\n16 GiB\n\n\nStorage\nEBS Only\n\n\nNetwork Performance\nUp to 10 Gigabit\n\n\nTotal Hourly Cost\n$0.193/hour ($0.154 + $0.039)\n\n\n\n\n\n\n\nOptimal memory-to-CPU ratio for Spark workloads\nARM instances provide 20% cost savings compared to x86\nSufficient capacity for concurrent small jobs\nProven performance for MSSQL/Iceberg operations\n\n\n\n\n\nm6a.xlarge: Recommended when x86 compatibility is required\nr6g.large: Suitable for memory-intensive UPSERT operations\nt3.xlarge: Cost-effective for sporadic workloads with burstable performance\n\n\n\n\nEMR provides multiple options for adjusting instance types as requirements evolve:\n\nFor Running Clusters\n\nUse Instance Fleets to mix different instance types\nGradually scale down old groups while scaling up new ones\nLeverage EMR’s automatic instance selection\n\nFor New Clusters\n\nSpecify new instance types in configuration\nUse temporary clusters for performance testing\nNo long-term commitment to specific instance types",
    "crumbs": [
      "Archive",
      "UPSERT Operations Cost Analysis"
    ]
  },
  {
    "objectID": "archive/upsert-ops-cost-analysis.html#key-differences",
    "href": "archive/upsert-ops-cost-analysis.html#key-differences",
    "title": "UPSERT Operations Cost Analysis: AWS Glue vs EMR",
    "section": "",
    "text": "AWS Glue\n\nPer-second billing at $0.44/DPU-Hour\n1-minute minimum billing increment\nAutomatic scaling based on workload\nNo infrastructure management costs\n\nEMR\n\nBilling based on cluster uptime\nSignificant savings available with Spot instances\nManual scaling control\nAdditional costs for cluster management\n\n\n\n\n\n\nAWS Glue\n\nZero infrastructure management\nNative integration with AWS CloudWatch\nLimited configuration options\nAutomatic version upgrades\n\nEMR\n\nComplete cluster control\nCustomizable configurations\nHigher maintenance requirements\nManual version management",
    "crumbs": [
      "Archive",
      "UPSERT Operations Cost Analysis"
    ]
  },
  {
    "objectID": "archive/upsert-ops-cost-analysis.html#backup-and-disaster-recovery",
    "href": "archive/upsert-ops-cost-analysis.html#backup-and-disaster-recovery",
    "title": "UPSERT Operations Cost Analysis: AWS Glue vs EMR",
    "section": "",
    "text": "Automatic script versioning\nIntegration with AWS Backup\nMulti-region deployment support\n99.9% availability SLA\n\n\n\n\n\nManual backup procedures required\nCustom DR solutions needed\nMulti-AZ deployment options\nNo standard SLA",
    "crumbs": [
      "Archive",
      "UPSERT Operations Cost Analysis"
    ]
  },
  {
    "objectID": "archive/upsert-ops-cost-analysis.html#security-and-compliance",
    "href": "archive/upsert-ops-cost-analysis.html#security-and-compliance",
    "title": "UPSERT Operations Cost Analysis: AWS Glue vs EMR",
    "section": "",
    "text": "Native AWS IAM integration\nBuilt-in encryption at rest\nVPC support\nAWS Shield protection\n\n\n\n\n\nCustomizable security groups\nManual encryption configuration\nVPC isolation required\nCustom security tools support",
    "crumbs": [
      "Archive",
      "UPSERT Operations Cost Analysis"
    ]
  },
  {
    "objectID": "archive/upsert-ops-cost-analysis.html#migration-considerations",
    "href": "archive/upsert-ops-cost-analysis.html#migration-considerations",
    "title": "UPSERT Operations Cost Analysis: AWS Glue vs EMR",
    "section": "",
    "text": "Export job definitions\nConvert to Spark code\nSetup cluster management\nEstimated time: 2-4 weeks\n\n\n\n\n\nConvert Spark code\nSetup AWS Glue crawlers\nConfigure job properties\nEstimated time: 1-3 weeks",
    "crumbs": [
      "Archive",
      "UPSERT Operations Cost Analysis"
    ]
  },
  {
    "objectID": "archive/upsert-ops-cost-analysis.html#performance-benchmarks",
    "href": "archive/upsert-ops-cost-analysis.html#performance-benchmarks",
    "title": "UPSERT Operations Cost Analysis: AWS Glue vs EMR",
    "section": "",
    "text": "Based on production workload testing:\n\n\n\nMetric\nAWS Glue\nEMR\n\n\n\n\nAvg. Processing Time\n45 mins\n38 mins\n\n\nCold Start Time\n2-3 mins\n8-10 mins\n\n\nMax Throughput\n100 GB/hr\n120 GB/hr\n\n\nConcurrent Jobs\n10\nUnlimited",
    "crumbs": [
      "Archive",
      "UPSERT Operations Cost Analysis"
    ]
  },
  {
    "objectID": "archive/upsert-ops-cost-analysis.html#recommendations",
    "href": "archive/upsert-ops-cost-analysis.html#recommendations",
    "title": "UPSERT Operations Cost Analysis: AWS Glue vs EMR",
    "section": "",
    "text": "Choose based on your primary requirements:\n\n\n\nTeam prefers managed services\nWorkload is sporadic or unpredictable\nQuick setup is prioritized (1-2 days)\nAWS-native tooling is preferred\nLimited DevOps resources available\n\n\n\n\n\nCost optimization is critical\nWorkload is consistent and predictable\nFine-grained control is required\nTeam has Spark expertise\nCustom configurations needed\n\n\n\n\n\nMixed workload characteristics\nDifferent teams have varying expertise\nCost optimization varies by workload\nGradual migration is preferred",
    "crumbs": [
      "Archive",
      "UPSERT Operations Cost Analysis"
    ]
  },
  {
    "objectID": "archive/upsert-ops-cost-analysis.html#references",
    "href": "archive/upsert-ops-cost-analysis.html#references",
    "title": "UPSERT Operations Cost Analysis: AWS Glue vs EMR",
    "section": "",
    "text": "AWS Glue Pricing (Accessed: January 2025)\nAmazon EMR Pricing (Accessed: January 2025)\nAWS Glue Best Practices\nEMR Benchmarking Guide\nAWS Big Data Blog: Glue vs EMR Comparison",
    "crumbs": [
      "Archive",
      "UPSERT Operations Cost Analysis"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html",
    "href": "archive/upsert-ops.html",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "",
    "text": "This document compares two AWS-based Apache Spark solutions for handling UPSERT operations from S3 to multiple target databases including MSSQL and Apache Iceberg:\n\nAWS Glue: A managed Apache Spark service\nApache Spark on Amazon EMR: A more configurable Spark deployment\n\n\n\n\n\n\n\nNote\n\n\n\nBoth solutions use Apache Spark as their processing engine. The key differences lie in management, configuration, and operational aspects rather than core processing capabilities.",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#aws-glue",
    "href": "archive/upsert-ops.html#aws-glue",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "AWS Glue",
    "text": "AWS Glue\nAWS Glue provides a managed Apache Spark environment with:\n\nBuilt-in Apache Spark engine (same as EMR)\nAWS-specific optimizations and tooling\nBoth Spark SQL and PySpark interfaces\nAdditional features like DynamicFrames\nManaged infrastructure and scaling",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#spark-on-emr",
    "href": "archive/upsert-ops.html#spark-on-emr",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "Spark on EMR",
    "text": "Spark on EMR\nAmazon EMR provides a more traditional Spark deployment with:\n\nFull Apache Spark ecosystem\nComplete configuration control\nCustom cluster management\nDirect access to Spark internals\nInfrastructure flexibility",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#aws-glue-costs",
    "href": "archive/upsert-ops.html#aws-glue-costs",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "3.1 AWS Glue Costs",
    "text": "3.1 AWS Glue Costs\n\nPricing StructureHidden SavingsConsiderations\n\n\n\n$0.44 per DPU-Hour (1 DPU = 4 vCPU, 16GB memory)\nMinimum 10-minute billing\nDevelopment endpoints additional cost\n\n\n\n\nNo cluster management costs\nIncludes Spark optimization\nLess operational overhead\n\n\n\n\nMore expensive per compute hour\nLess granular scaling\nSimplified cost model",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#emr-costs",
    "href": "archive/upsert-ops.html#emr-costs",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "3.2 EMR Costs",
    "text": "3.2 EMR Costs\n\nDirect CostsOptimization OptionsHidden Costs\n\n\n\nEC2 instance costs\nEMR service charges\nStorage and data transfer\n\n\n\n\nSpot instance usage\nMore granular scaling\nResource optimization\n\n\n\n\nOperational overhead\nManagement complexity\nRequired expertise",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#aws-glue-performance",
    "href": "archive/upsert-ops.html#aws-glue-performance",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "4.1 AWS Glue Performance",
    "text": "4.1 AWS Glue Performance\n# Example Glue Spark UPSERT implementation\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\n\n# Initialize Glue Spark context\nglueContext = GlueContext(SparkContext.getOrCreate())\nspark = glueContext.spark_session\n\n# Read from S3 (using standard Spark)\nsource_df = spark.read.parquet(\"s3://bucket/path\")\n\n# MSSQL UPSERT\ndef perform_mssql_upsert(df):\n    # Write to staging table using Spark JDBC\n    df.write \\\n        .format(\"jdbc\") \\\n        .option(\"url\", jdbc_url) \\\n        .option(\"dbtable\", \"staging_table\") \\\n        .mode(\"overwrite\") \\\n        .save()\n\n    # Execute MERGE using Spark SQL\n    spark.sql(\"\"\"\n    MERGE INTO target_table t\n    USING staging_table s\n    ON t.key = s.key\n    WHEN MATCHED THEN UPDATE...\n    WHEN NOT MATCHED THEN INSERT...\n    \"\"\")\n\n\n\n\n\n\nGlue Performance Strengths\n\n\n\n\nPre-configured Spark optimizations\nAWS service-specific tuning\nAuto-scaling built in\nWarm pools reduce startup time\n\n\n\n\n\n\n\n\n\nGlue Performance Limitations\n\n\n\n\nLess Spark configuration flexibility\nFixed worker configurations\nLimited Spark version control",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#emr-performance",
    "href": "archive/upsert-ops.html#emr-performance",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "4.2 EMR Performance",
    "text": "4.2 EMR Performance\n# Example EMR Spark UPSERT implementation\nfrom pyspark.sql import SparkSession\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"EMR UPSERT\") \\\n    .getOrCreate()\n\n# Read from S3 (identical to Glue)\nsource_df = spark.read.parquet(\"s3://bucket/path\")\n\n# MSSQL UPSERT (identical to Glue)\ndef perform_mssql_upsert(df):\n    df.write \\\n        .format(\"jdbc\") \\\n        .option(\"url\", jdbc_url) \\\n        .option(\"dbtable\", \"staging_table\") \\\n        .mode(\"overwrite\") \\\n        .save()\n\n\n\n\n\n\nEMR Performance Strengths\n\n\n\n\nFull Spark configuration control\nCustom Spark properties\nBetter performance for large jobs\nFine-grained optimization\n\n\n\n\n\n\n\n\n\nEMR Performance Limitations\n\n\n\n\nRequires Spark expertise\nInfrastructure management overhead\nCluster startup time",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#key-differences",
    "href": "archive/upsert-ops.html#key-differences",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "5.1 Key Differences",
    "text": "5.1 Key Differences\n\n\n\nAspect\nAWS Glue\nSpark on EMR\n\n\n\n\nSetup Complexity\nLow\nHigh\n\n\nConfiguration Options\nLimited\nExtensive\n\n\nDevelopment Tools\nAWS Console + IDE\nAny IDE\n\n\nLocal Testing\nLimited\nFull Support\n\n\nDebugging\nBasic\nAdvanced",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#integration-capabilities",
    "href": "archive/upsert-ops.html#integration-capabilities",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "5.2 Integration Capabilities",
    "text": "5.2 Integration Capabilities\n\nAWS GlueSpark on EMR\n\n\n\nNative AWS integration\nPre-configured connectors\nStandard Spark JDBC\nBasic Iceberg support\n\n\n\n\nFull connector ecosystem\nCustom connectors\nAll Spark data sources\nComplete Iceberg support",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#monitoring-options",
    "href": "archive/upsert-ops.html#monitoring-options",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "6.1 Monitoring Options",
    "text": "6.1 Monitoring Options\n\nAWS GlueSpark on EMR\n\n\n\nCloudWatch integration\nBuilt-in dashboards\nAuto-retry capability\nAWS-native alerting\n\n\n\n\nFull Spark metrics\nCustom monitoring\nDetailed job tracking\nThird-party tools",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#operational-requirements",
    "href": "archive/upsert-ops.html#operational-requirements",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "6.2 Operational Requirements",
    "text": "6.2 Operational Requirements\n\n\n\nRequirement\nAWS Glue\nSpark on EMR\n\n\n\n\nSpark Expertise\nBasic\nAdvanced\n\n\nDevOps Support\nMinimal\nSubstantial\n\n\nMaintenance\nAWS Managed\nSelf Managed\n\n\nScaling\nAutomatic\nManual/Custom",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#short-term",
    "href": "archive/upsert-ops.html#short-term",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "7.1 Short Term",
    "text": "7.1 Short Term\nRecommend starting with AWS Glue due to:\n\nFaster implementation\nManaged environment\nSufficient for current scale\nLower operational overhead",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#long-term",
    "href": "archive/upsert-ops.html#long-term",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "7.2 Long Term",
    "text": "7.2 Long Term\nConsider migration to EMR if:\n\nApproaching cost crossover point\nRequiring more performance optimization\nTeam has built Spark expertise\nNeed more control over infrastructure",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#phase-1-initial-setup",
    "href": "archive/upsert-ops.html#phase-1-initial-setup",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "8.1 Phase 1: Initial Setup",
    "text": "8.1 Phase 1: Initial Setup\n\nSet up development environment\nCreate test jobs\nEstablish monitoring\nDocument procedures",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#phase-2-production-migration",
    "href": "archive/upsert-ops.html#phase-2-production-migration",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "8.2 Phase 2: Production Migration",
    "text": "8.2 Phase 2: Production Migration\n\nMigrate simple jobs\nAdd error handling\nImplement monitoring\nDocument operations",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "archive/upsert-ops.html#phase-3-optimization",
    "href": "archive/upsert-ops.html#phase-3-optimization",
    "title": "AWS Glue vs Spark on EMR for UPSERT Operations",
    "section": "8.3 Phase 3: Optimization",
    "text": "8.3 Phase 3: Optimization\n\nPerformance tuning\nCost optimization\nProcess refinement\nTeam training\n\n\n\n\n\n\n\nNext Steps\n\n\n\n\nPrototype both solutions\nTest with production data volumes\nCalculate actual costs",
    "crumbs": [
      "Archive",
      "UPSERT Operations"
    ]
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html",
    "href": "erp-extraction-ingestion-vbd.html",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "",
    "text": "The system must be able to connect to various ERP systems (e.g., Epicor, Sage) via their APIs, authenticate, and extract required data types. This includes:\n\nAuthenticating with proper credentials (API keys, tokens, client certificates)\nBuilding appropriate requests for each ERP system\nHandling pagination and batching for large datasets\nManaging rate limits and connection timeouts\nCapturing and processing response data\nImplementing resilience patterns for API communication\n\n\n\n\nFor ERP systems that allow or require direct database access, the system must:\n\nConnect securely to various database types\nExecute appropriate queries for different data types\nHandle connection pooling and resource management\nApply appropriate filtering for incremental extraction\nProcess result sets efficiently\nRelease database resources properly\n\n\n\n\nAll extracted data must undergo initial transformation to:\n\nStandardize column names according to our global data dictionary\nValidate data against defined schemas and quality rules\nConvert to Parquet format for efficient storage and processing\nApply optional compression for reduced storage needs\nIdentify and mask sensitive information when required\nPreserve data lineage information\n\n\n\n\nThe system must securely manage credentials for various ERP systems:\n\nRetrieve credentials from HashiCorp Vault using least privilege access\nSupport different credential types (API keys, database credentials, certificates)\nHandle credential rotation and expiration\nEnsure credentials are never logged or persisted outside secure storage\nApply proper authentication mechanisms for each ERP type\n\n\n\n\nFor audit and compliance purposes, the system must:\n\nTrack the source, extraction time, and parameters of each data extraction\nRecord transformation details and any quality issues\nLink extracted data to its final storage location\nProvide metrics on extraction volume and timing\nEnable traceability for data governance\n\n\n\n\nThe system must provide comprehensive monitoring:\n\nReport on successful and failed extractions\nExpose metrics for performance and throughput\nAlert on critical failures or data quality issues\nLog detailed information for troubleshooting\nProvide health check endpoints for operational status"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#extract-data-from-erp-systems-via-api",
    "href": "erp-extraction-ingestion-vbd.html#extract-data-from-erp-systems-via-api",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "",
    "text": "The system must be able to connect to various ERP systems (e.g., Epicor, Sage) via their APIs, authenticate, and extract required data types. This includes:\n\nAuthenticating with proper credentials (API keys, tokens, client certificates)\nBuilding appropriate requests for each ERP system\nHandling pagination and batching for large datasets\nManaging rate limits and connection timeouts\nCapturing and processing response data\nImplementing resilience patterns for API communication"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#extract-data-from-erp-databases",
    "href": "erp-extraction-ingestion-vbd.html#extract-data-from-erp-databases",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "",
    "text": "For ERP systems that allow or require direct database access, the system must:\n\nConnect securely to various database types\nExecute appropriate queries for different data types\nHandle connection pooling and resource management\nApply appropriate filtering for incremental extraction\nProcess result sets efficiently\nRelease database resources properly"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#transform-extracted-data",
    "href": "erp-extraction-ingestion-vbd.html#transform-extracted-data",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "",
    "text": "All extracted data must undergo initial transformation to:\n\nStandardize column names according to our global data dictionary\nValidate data against defined schemas and quality rules\nConvert to Parquet format for efficient storage and processing\nApply optional compression for reduced storage needs\nIdentify and mask sensitive information when required\nPreserve data lineage information"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#secure-credential-management",
    "href": "erp-extraction-ingestion-vbd.html#secure-credential-management",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "",
    "text": "The system must securely manage credentials for various ERP systems:\n\nRetrieve credentials from HashiCorp Vault using least privilege access\nSupport different credential types (API keys, database credentials, certificates)\nHandle credential rotation and expiration\nEnsure credentials are never logged or persisted outside secure storage\nApply proper authentication mechanisms for each ERP type"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#track-data-lineage",
    "href": "erp-extraction-ingestion-vbd.html#track-data-lineage",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "",
    "text": "For audit and compliance purposes, the system must:\n\nTrack the source, extraction time, and parameters of each data extraction\nRecord transformation details and any quality issues\nLink extracted data to its final storage location\nProvide metrics on extraction volume and timing\nEnable traceability for data governance"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#monitor-and-report-system-health",
    "href": "erp-extraction-ingestion-vbd.html#monitor-and-report-system-health",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "",
    "text": "The system must provide comprehensive monitoring:\n\nReport on successful and failed extractions\nExpose metrics for performance and throughput\nAlert on critical failures or data quality issues\nLog detailed information for troubleshooting\nProvide health check endpoints for operational status"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#api-data-extraction-workflow",
    "href": "erp-extraction-ingestion-vbd.html#api-data-extraction-workflow",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "2.1 API Data Extraction Workflow",
    "text": "2.1 API Data Extraction Workflow\n\nInitiation:\n\nSystem receives extraction command with parameters (ERP type, client ID, data type)\nValidate parameters and check authorization\n\nPreparation:\n\nRetrieve ERP configuration from FerretDB\nObtain appropriate credentials from HashiCorp Vault\nDetermine extraction mode (full or incremental)\nInitialize data lineage tracking\n\nRequest Construction:\n\nBuild appropriate API request with authentication\nApply ERP-specific headers and parameters\nConfigure timeouts and retry settings\n\nExecution:\n\nSend request to ERP API endpoint\nHandle pagination for large datasets\nApply circuit breaker pattern for resilience\nProcess response data into standardized format\n\nTransformation:\n\nMap column names to standard dictionary\nValidate data against quality rules\nMask sensitive information if required\nConvert to Parquet format\n\nStorage:\n\nUpload transformed data to pre-dropzone S3 bucket\nOrganize by client, ERP type, and data type\nApply appropriate permissions\n\nCompletion:\n\nUpdate data lineage with completion status\nLog extraction metrics and results\nReport success or failure"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#database-data-extraction-workflow",
    "href": "erp-extraction-ingestion-vbd.html#database-data-extraction-workflow",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "2.2 Database Data Extraction Workflow",
    "text": "2.2 Database Data Extraction Workflow\n\nInitiation:\n\nSystem receives extraction command with parameters\nValidate parameters and check authorization\n\nPreparation:\n\nRetrieve ERP database configuration\nObtain database credentials from Vault\nDetermine extraction mode (full or incremental)\nInitialize data lineage tracking\n\nQuery Construction:\n\nBuild appropriate SQL query for the data type\nApply filters for incremental extraction if needed\nSet appropriate query timeout and batch size\n\nExecution:\n\nEstablish database connection with proper isolation\nExecute query with bulkhead protection\nStream results to minimize memory usage\nMap database results to standardized format\n\nTransformation:\n\n(Same as API workflow)\n\nStorage:\n\n(Same as API workflow)\n\nCompletion:\n\n(Same as API workflow)"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#clients-layer",
    "href": "erp-extraction-ingestion-vbd.html#clients-layer",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "3.1 Clients Layer",
    "text": "3.1 Clients Layer\nThe clients layer serves as the entry point for the extraction process, receiving commands and initiating the workflow.\n\n3.1.1 Components\n\nProgram Class:\n\nParses command-line arguments\nConfigures dependency injection\nInitializes and starts the host\n\nExtractionHostedService:\n\nServes as the main entry point for the extraction process\nDelegates to appropriate managers\nHandles top-level error reporting and exit codes"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#managers-layer",
    "href": "erp-extraction-ingestion-vbd.html#managers-layer",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "3.2 Managers Layer",
    "text": "3.2 Managers Layer\nThe managers layer orchestrates the workflows, coordinating between different components without containing business logic.\n\n3.2.1 Components\n\nExtractionManager:\n\nCoordinates the overall extraction process\nDelegates to other managers and engines\nTracks progress and ensures completion\nHandles exceptions and ensures proper cleanup\n\nSecurityManager:\n\nManages credential retrieval and security concerns\nEnsures least privilege access to resources\nHandles rotation and expiration of credentials\n\nConfigurationManager:\n\nRetrieves and manages ERP configuration\nDetermines correct extraction mode and settings\nConfigures feature flags and system behavior"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#engines-layer",
    "href": "erp-extraction-ingestion-vbd.html#engines-layer",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "3.3 Engines Layer",
    "text": "3.3 Engines Layer\nThe engines layer contains the core business logic, implementing the rules and processes that define the system’s behavior.\n\n3.3.1 Components\n\nExtractorEngine:\n\nImplements data extraction logic for different sources\nBuilds appropriate requests or queries\nProcesses extracted data into standardized format\nApplies ERP-specific extraction rules\n\nTransformerEngine:\n\nStandardizes column names per data dictionary\nApplies data type conversions\nImplements transformation rules\nGenerates Parquet output\n\nValidationEngine:\n\nValidates data against schemas and rules\nIdentifies quality issues\nReports validation results\nEnforces data governance policies\n\nLineageEngine:\n\nTracks data provenance and transformations\nRecords extraction parameters and results\nMaintains audit trail for compliance\nLinks extracted data to downstream processes"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#accessors-layer",
    "href": "erp-extraction-ingestion-vbd.html#accessors-layer",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "3.4 Accessors Layer",
    "text": "3.4 Accessors Layer\nThe accessors layer handles all interactions with external systems and resources.\n\n3.4.1 Components\n\nVaultAccessor:\n\nInteracts with HashiCorp Vault\nRetrieves and manages secrets\nApplies caching and resilience patterns\nEnsures secure credential handling\n\nFerretDBAccessor:\n\nConnects to FerretDB (MongoDB protocol)\nRetrieves configuration documents\nManages database connections efficiently\nApplies retry and circuit breaker patterns\n\nERPApiAccessor:\n\nHandles HTTP communication with ERP APIs\nApplies authentication and security measures\nManages request/response lifecycle\nImplements resilience patterns for API calls\n\nERPDatabaseAccessor:\n\nConnects to ERP databases\nExecutes queries and processes results\nManages database resources efficiently\nApplies connection pooling and isolation\n\nS3Accessor:\n\nUploads data to S3 storage\nManages bucket permissions and organization\nEnsures data integrity during transfer\nHandles S3 API interactions"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#resources-layer",
    "href": "erp-extraction-ingestion-vbd.html#resources-layer",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "3.5 Resources Layer",
    "text": "3.5 Resources Layer\nThe resources layer represents external systems and dependencies that our system interacts with.\n\n3.5.1 Components\n\nHashiCorp Vault:\n\nSecure storage for credentials and secrets\nProvides short-lived, least-privilege access tokens\n\nFerretDB:\n\nConfiguration storage using MongoDB protocol\nStores ERP-specific settings and parameters\n\nERP APIs:\n\nExternal API endpoints for various ERP systems\nProvides data access via HTTP/HTTPS\n\nERP Databases:\n\nDirect database connections to ERP systems\nProvides data access via SQL queries\n\nS3 Storage:\n\nPre-dropzone bucket for extracted data\nDestination for transformed Parquet files"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#phase-1-core-extraction-capability",
    "href": "erp-extraction-ingestion-vbd.html#phase-1-core-extraction-capability",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "4.1 Phase 1: Core Extraction Capability",
    "text": "4.1 Phase 1: Core Extraction Capability\n\nSetup Project Structure:\n\nCreate solution with projects for each layer\nSet up dependency injection framework\nImplement basic interfaces and models\n\nImplement Initial Vertical Slice:\n\nFocus on one ERP type (Epicor API)\nImplement minimal components in each layer\nCreate end-to-end workflow for basic extraction\n\nAdd Basic Validation and Transformation:\n\nImplement column name standardization\nAdd simple data quality checks\nConvert to Parquet format"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#phase-2-expand-erp-support",
    "href": "erp-extraction-ingestion-vbd.html#phase-2-expand-erp-support",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "4.2 Phase 2: Expand ERP Support",
    "text": "4.2 Phase 2: Expand ERP Support\n\nAdd Database Extraction:\n\nImplement database accessor\nAdd query building functionality\nSupport direct database extraction\n\nAdd Additional ERP Types:\n\nImplement Sage connector\nSupport for other common ERP systems\nCreate factory functions for dynamic resolution\n\nEnhance Transformation Capabilities:\n\nImplement more advanced data validation\nAdd field-level masking for sensitive data\nSupport incremental extraction with CDC"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#phase-3-resilience-and-monitoring",
    "href": "erp-extraction-ingestion-vbd.html#phase-3-resilience-and-monitoring",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "4.3 Phase 3: Resilience and Monitoring",
    "text": "4.3 Phase 3: Resilience and Monitoring\n\nImplement Resilience Patterns:\n\nAdd circuit breakers for external dependencies\nImplement retry with exponential backoff\nAdd bulkhead isolation for critical operations\n\nAdd Monitoring and Metrics:\n\nImplement OpenTelemetry integration\nAdd performance metrics collection\nCreate health check endpoints\n\nEnhance Error Handling:\n\nImplement comprehensive exception handling\nAdd detailed logging for troubleshooting\nCreate structured error reporting"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#phase-4-security-and-compliance",
    "href": "erp-extraction-ingestion-vbd.html#phase-4-security-and-compliance",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "4.4 Phase 4: Security and Compliance",
    "text": "4.4 Phase 4: Security and Compliance\n\nEnhance Security Features:\n\nImplement mutual TLS for applicable ERPs\nAdd field-level encryption\nEnhance credential management\n\nComplete Data Lineage:\n\nImplement comprehensive lineage tracking\nAdd audit logging for compliance\nCreate lineage visualization tools\n\nDocumentation and Training:\n\nFinalize system documentation\nCreate operator guides and runbooks\nConduct training sessions for support teams"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#iextractionmanager",
    "href": "erp-extraction-ingestion-vbd.html#iextractionmanager",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "5.1 IExtractionManager",
    "text": "5.1 IExtractionManager\n/// &lt;summary&gt;\n/// Manages the extraction workflow for ERP data\n/// &lt;/summary&gt;\npublic interface IExtractionManager\n{\n    /// &lt;summary&gt;\n    /// Extracts data from the specified ERP system\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"erpType\"&gt;The type of ERP system&lt;/param&gt;\n    /// &lt;param name=\"clientId\"&gt;The client identifier&lt;/param&gt;\n    /// &lt;param name=\"dataType\"&gt;The type of data to extract&lt;/param&gt;\n    /// &lt;param name=\"cancellationToken\"&gt;Cancellation token&lt;/param&gt;\n    /// &lt;returns&gt;A task representing the extraction operation&lt;/returns&gt;\n    Task ExtractDataAsync(\n        string erpType, \n        string clientId, \n        string dataType, \n        CancellationToken cancellationToken);\n}"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#isecuritymanager",
    "href": "erp-extraction-ingestion-vbd.html#isecuritymanager",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "5.2 ISecurityManager",
    "text": "5.2 ISecurityManager\n/// &lt;summary&gt;\n/// Manages security concerns including credential retrieval\n/// &lt;/summary&gt;\npublic interface ISecurityManager\n{\n    /// &lt;summary&gt;\n    /// Gets credentials for the specified ERP system\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"erpType\"&gt;The type of ERP system&lt;/param&gt;\n    /// &lt;param name=\"clientId\"&gt;The client identifier&lt;/param&gt;\n    /// &lt;param name=\"dataType\"&gt;The type of data to extract&lt;/param&gt;\n    /// &lt;returns&gt;Credentials for the specified ERP system&lt;/returns&gt;\n    Task&lt;Credentials&gt; GetCredentialsAsync(\n        string erpType, \n        string clientId, \n        string dataType);\n}"
  },
  {
    "objectID": "erp-extraction-ingestion-vbd.html#iextractorengine",
    "href": "erp-extraction-ingestion-vbd.html#iextractorengine",
    "title": "Design Document for Distributor Data Extraction Ingestion - VBD",
    "section": "5.3 IExtractorEngine",
    "text": "5.3 IExtractorEngine\n/// &lt;summary&gt;\n/// Implements core extraction logic for different ERP systems\n/// &lt;/summary&gt;\npublic interface IExtractorEngine\n{\n    /// &lt;summary&gt;\n    /// Extracts data from the specified ERP system\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"erpType\"&gt;The type of ERP system&lt;/param&gt;\n    /// &lt;param name=\"config\"&gt;Extraction configuration&lt;/param&gt;\n    /// &lt;param name=\"credentials\"&gt;ERP credentials&lt;/param&gt;\n    /// &lt;param name=\"cancellationToken\"&gt;Cancellation token&lt;/param&gt;\n    /// &lt;returns&gt;The extracted data&lt;/returns&gt;\n    Task&lt;ExtractedData&gt; ExtractAsync(\n        string erpType, \n        ExtractorConfig config, \n        Credentials credentials, \n        CancellationToken cancellationToken);\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ReSight: Building the Future of Pet Industry Analytics",
    "section": "",
    "text": "ReSight is positioning itself to become the authoritative source of truth and insights for the U.S. pet industry. This transformation requires a robust, scalable ETL infrastructure capable of processing comprehensive industry data at scale.\n\n\nOur current ETL infrastructure demonstrates significant daily processing capacity with notable variability in workload:\n\n\n\n\n\nMetric\nTypical Day (Median)\nPeak Day\nAverage (Mean)\n\n\n\n\nLoads Processed\n40\n303\n59.2\n\n\nRows Processed\n70,588\n824,719\n105,218\n\n\nProcessing Window\nFlexible\nFlexible\nFlexible\n\n\n\n\n\n\n\nDaily Load Range: 1-303 loads per day\nTypical Range (Q1-Q3): 24-65 loads per day\nStandard Deviation: 54.8 loads, indicating high variability\nProcessing Reliability: 361 days of consistent operation with no outages\n\n\n\n\n\nDaily Row Range: 28-824,719 rows\nTypical Range (Q1-Q3): 15,606-159,225 rows\nVolume Variability: Standard deviation of 115,282 rows\nProcessing Success Rate: 100% (no missing days)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric\nValue\nGrowth Factor\n\n\n\n\nDaily Loads\n400+/day\n10x current median\n\n\nData Volume\n700K+ rows/day typical\n10x current median\n\n\nData Sources\n1000+ integrated sources\n10x current scale\n\n\nComplexity\nHigh (ML pipelines)\nSignificant increase\n\n\nProcessing Window\nNear real-time requirements (if needed)\nMinutes vs. flexible\n\n\n\n\n\n\nOur next-generation ETL pipeline must support:\n\nScalable Data Integration\n\nHandle 10x increase in daily load frequency\nProcess 10x current data volumes\nSupport 10x growth in data source connections\n\nAdvanced Processing Capabilities\n\nPredictive analytics pipelines\nMachine learning model integration\nMarket insight generation\n\nEnterprise-Scale Operations\n\nConsistent high-volume processing\n24/7 operation with high availability\nIndustry-leading security controls",
    "crumbs": [
      "Archive",
      "Dev Plan and Overview"
    ]
  },
  {
    "objectID": "index.html#current-state-analysis-2024",
    "href": "index.html#current-state-analysis-2024",
    "title": "ReSight: Building the Future of Pet Industry Analytics",
    "section": "",
    "text": "Our current ETL infrastructure demonstrates significant daily processing capacity with notable variability in workload:\n\n\n\n\n\nMetric\nTypical Day (Median)\nPeak Day\nAverage (Mean)\n\n\n\n\nLoads Processed\n40\n303\n59.2\n\n\nRows Processed\n70,588\n824,719\n105,218\n\n\nProcessing Window\nFlexible\nFlexible\nFlexible\n\n\n\n\n\n\n\nDaily Load Range: 1-303 loads per day\nTypical Range (Q1-Q3): 24-65 loads per day\nStandard Deviation: 54.8 loads, indicating high variability\nProcessing Reliability: 361 days of consistent operation with no outages\n\n\n\n\n\nDaily Row Range: 28-824,719 rows\nTypical Range (Q1-Q3): 15,606-159,225 rows\nVolume Variability: Standard deviation of 115,282 rows\nProcessing Success Rate: 100% (no missing days)",
    "crumbs": [
      "Archive",
      "Dev Plan and Overview"
    ]
  },
  {
    "objectID": "index.html#target-state-2026",
    "href": "index.html#target-state-2026",
    "title": "ReSight: Building the Future of Pet Industry Analytics",
    "section": "",
    "text": "Metric\nValue\nGrowth Factor\n\n\n\n\nDaily Loads\n400+/day\n10x current median\n\n\nData Volume\n700K+ rows/day typical\n10x current median\n\n\nData Sources\n1000+ integrated sources\n10x current scale\n\n\nComplexity\nHigh (ML pipelines)\nSignificant increase\n\n\nProcessing Window\nNear real-time requirements (if needed)\nMinutes vs. flexible",
    "crumbs": [
      "Archive",
      "Dev Plan and Overview"
    ]
  },
  {
    "objectID": "index.html#growth-requirements",
    "href": "index.html#growth-requirements",
    "title": "ReSight: Building the Future of Pet Industry Analytics",
    "section": "",
    "text": "Our next-generation ETL pipeline must support:\n\nScalable Data Integration\n\nHandle 10x increase in daily load frequency\nProcess 10x current data volumes\nSupport 10x growth in data source connections\n\nAdvanced Processing Capabilities\n\nPredictive analytics pipelines\nMachine learning model integration\nMarket insight generation\n\nEnterprise-Scale Operations\n\nConsistent high-volume processing\n24/7 operation with high availability\nIndustry-leading security controls",
    "crumbs": [
      "Archive",
      "Dev Plan and Overview"
    ]
  },
  {
    "objectID": "index.html#core-requirements",
    "href": "index.html#core-requirements",
    "title": "ReSight: Building the Future of Pet Industry Analytics",
    "section": "2.1 Core Requirements",
    "text": "2.1 Core Requirements\n\n2.1.1 Scalability\n\nSupport for 400+ daily loads (10x current median)\nPeak capacity of 8M+ rows per day\nElastic resource allocation\nHorizontal scaling support\n\n\n\n2.1.2 Advanced Analytics\n\nML pipeline integration\nComplex data transformations\nData science toolkit support\nPredictive modeling capability\n\n\n\n2.1.3 Reliability\n\nZero downtime (matching current 100% reliability)\nAutomated failover\nComprehensive monitoring\nProactive scaling",
    "crumbs": [
      "Archive",
      "Dev Plan and Overview"
    ]
  },
  {
    "objectID": "index.html#key-performance-indicators",
    "href": "index.html#key-performance-indicators",
    "title": "ReSight: Building the Future of Pet Industry Analytics",
    "section": "2.2 Key Performance Indicators",
    "text": "2.2 Key Performance Indicators\n\n\n\nMetric\nCurrent (2024)\nTarget (2026)\n\n\n\n\nDaily Loads (Median)\n40\n400+\n\n\nPeak Daily Loads\n303\n3000+\n\n\nDaily Rows (Median)\n70,588\n700K+\n\n\nPeak Daily Rows\n824,719\n8M+\n\n\nProcessing Latency\nHours\nMinutes\n\n\nData Sources\n~100\n1000+",
    "crumbs": [
      "Archive",
      "Dev Plan and Overview"
    ]
  },
  {
    "objectID": "index.html#phase-1-foundation-q1-2025",
    "href": "index.html#phase-1-foundation-q1-2025",
    "title": "ReSight: Building the Future of Pet Industry Analytics",
    "section": "3.1 Phase 1: Foundation (Q1 2025)",
    "text": "3.1 Phase 1: Foundation (Q1 2025)\n\nScale current infrastructure to handle 2x current peak load\nImplement comprehensive monitoring\nDeploy new stream processing architecture",
    "crumbs": [
      "Archive",
      "Dev Plan and Overview"
    ]
  },
  {
    "objectID": "index.html#phase-2-scaling-q2-q3-2025",
    "href": "index.html#phase-2-scaling-q2-q3-2025",
    "title": "ReSight: Building the Future of Pet Industry Analytics",
    "section": "3.2 Phase 2: Scaling (Q2-Q3 2025)",
    "text": "3.2 Phase 2: Scaling (Q2-Q3 2025)\n\nExpand data source integration capacity\nImplement ML pipeline framework",
    "crumbs": [
      "Archive",
      "Dev Plan and Overview"
    ]
  },
  {
    "objectID": "index.html#phase-3-optimization-q4-2025",
    "href": "index.html#phase-3-optimization-q4-2025",
    "title": "ReSight: Building the Future of Pet Industry Analytics",
    "section": "3.3 Phase 3: Optimization (Q4 2025)",
    "text": "3.3 Phase 3: Optimization (Q4 2025)\n\nScale to 5x current capacity\nDeploy advanced analytics capabilities",
    "crumbs": [
      "Archive",
      "Dev Plan and Overview"
    ]
  },
  {
    "objectID": "index.html#phase-4-enterprise-scale-2026",
    "href": "index.html#phase-4-enterprise-scale-2026",
    "title": "ReSight: Building the Future of Pet Industry Analytics",
    "section": "3.4 Phase 4: Enterprise Scale (2026)",
    "text": "3.4 Phase 4: Enterprise Scale (2026)\n\nAchieve full target state capabilities\nDeploy full ML/AI integration",
    "crumbs": [
      "Archive",
      "Dev Plan and Overview"
    ]
  },
  {
    "objectID": "archive/etl-pipeline-planning-questions.html",
    "href": "archive/etl-pipeline-planning-questions.html",
    "title": "ETL Pipeline Planning Questions",
    "section": "",
    "text": "1 Data Source Access\n\n1.0.1 How will you handle different API rate limits across these diverse systems?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe’ll implement a centralized rate limiting solution using DynamoDB to store and manage rate limits for each ERP system. Our approach includes:\nA DynamoDB table storing each ERP’s configuration including:\nRate limits and burst limits Contact information Alert thresholds Version history Approval status\nAWS API Gateway for enforcing these limits, with usage plans dynamically configured based on the DynamoDB data. A Lambda function that syncs DynamoDB configurations with API Gateway usage plans, ensuring rate limits are always up to date. CloudWatch monitoring to track API usage against limits, with automated alerts at configurable thresholds (e.g., 80% warning, 95% critical). An admin API for managing rate limits with an approval workflow for any changes.\nThis solution provides centralized management, version control, audit trails, and automated monitoring - all essential for managing multiple ERP integrations at scale.”\n\n\n\n\n\n1.0.2 What’s your backup plan for ERPs that don’t provide API access?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor our ETL pipeline, we have specific strategies for each type of ERP system:\nAPI-Based ERPs:\n\nImplement centralized rate limiting using DynamoDB to store API configurations\nUse AWS API Gateway to manage and enforce rate limits\nSet up CloudWatch monitoring for usage tracking\nDeploy Lambda functions for each API integration\n\nDatabase-Driven ERPs:\n\nUse AWS Database Migration Service (DMS) for continuous replication\nSet up read replicas to minimize load on source systems\nImplement Change Data Capture (CDC) to track only changed records\nUse AWS Secrets Manager for database credentials\n\nFile-Based ERPs:\n\nCreate dedicated S3 buckets for each system\nUse AWS Transfer Family for secure SFTP access\nDeploy Lambda functions to monitor and process new files\nImplement file integrity checks and validation\n\nLegacy Systems:\n\nCustom extract programs for regular data exports\nScheduled batch processes for data extraction\nFile-based transfers to S3 using AWS Transfer Family\nValidation checks for data completeness\n\nCommon Infrastructure Across All Types:\n\nCloudWatch monitoring and alerting\nData validation before loading\nAudit logging of all transfers\nError handling and retry mechanisms\nCentralized credential management using AWS Secrets Manager\n\n\n\n\n\n\n1.0.3 How will you maintain and secure credentials for 30+ different systems?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe’ll implement a two-part credential management system:\n\nAWS Secrets Manager for sensitive credentials:\n\nAll ERP credentials (API keys, passwords)\nAutomatic credential rotation\nBuilt-in encryption using KMS\nIAM role-based access control\n\nDynamoDB for configuration and references:\n\n{\n    \"erpId\": \"erp_a\",\n    \"version\": \"v1\",\n    \"secretArn\": \"arn:aws:secretsmanager:region:account:secret:erp-a-credentials\",\n    \"integration\": {\n        \"type\": \"API-Based\",\n        \"method\": \"REST\"\n    },\n    \"metadata\": {\n        \"owner\": \"team_a\",\n        \"contact\": \"team_a@company.com\"\n    }\n}\nLambda functions access credentials by:\n\nFetching configuration from DynamoDB\nUsing secretArn to retrieve actual credentials from Secrets Manager\nEach Lambda has IAM roles with least-privilege access\n\nThis provides:\n\nSecure credential storage\nCentral configuration management\nAudit logging through CloudWatch\nAutomated credential rotation\nVersion control of configurations”\n\n\n\n\n\n\n1.0.4 How will you handle API/connection failures?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSince our ETL pipeline runs once daily rather than real-time, our failure handling focuses on job completion certainty:\nClear Job Status Tracking\n\nEach daily job has a unique identifier\nWe track success/failure status for each ERP system\nAll errors are logged with specific failure reasons\n\nFailure Recovery Process\n\nFailed jobs automatically retry up to 3 times\nOne hour delay between retry attempts\nAfter all retries fail, triggers team notification\nManual investigation required before next day’s run\n\nValidation Checks\n\nVerify data completeness before marking job as successful\nCompare record counts with expected ranges\nCheck for data quality issues\n\nMonitoring & Alerts\n\nDaily job completion status dashboard\nImmediate alerts for failed jobs\nHistorical job status tracking for pattern analysis\n\nThis approach ensures we always know whether each day’s data fetch succeeded or failed, and exactly why any failures occurred.\n\n\n\n\n\n\n2 Data Standardization\n\n2.0.1 Have you mapped the schema differences between all these ERPs?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe manage schema differences through a standardized mapping approach:\nCentral Data Dictionary\n\nMaintain a master schema in DynamoDB defining our standard format\nEach ERP has a mapping configuration showing how its fields translate to our standard\nInclude data type conversions and formatting rules\n\nData Standardization Process\n\nRaw data is first stored in its original format\nTransformation layer converts to our standard schema\nAll dates normalized to UTC\nConsistent naming conventions applied\nField values standardized (e.g., ‘Y’/‘N’ to true/false)\n\nQuality Assurance\n\nAutomated validation of transformed data\nAlerts for unexpected data formats\nTrack any fields that can’t be mapped\nRegular audits of mapping accuracy\n\n\n\n\n\n\n2.0.2 How will you handle inconsistent field names/data types across systems?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe handle field name and data type inconsistencies through a transformation layer:\nField Name Standardization\n\nUse mapping tables in DynamoDB to define standard names\nEach ERP entry includes:\n\nOriginal field name\nStandardized field name\nBusiness context\nRequired transformations\n\n\nData Type Handling\n\nNumber formats (handle different decimal separators)\nDate/time formats (normalize to UTC)\nText encodings (convert to UTF-8)\nBoolean values (convert Y/N, 1/0, True/False to standard boolean)\n\nValidation Rules\n\nRequired fields checking\nData type verification\nRange/format validation\nBusiness rule validation\n\nError Handling\n\nTag records with transformation errors\nKeep original values for audit\nAlert on systematic conversion issues\nMaintain transformation logs\n\n\n\n\n\n\n2.0.3 What’s your plan for handling different date/time formats from various systems?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe standardize all date/time handling through a consistent process:\nDate/Time Standardization\n\nConvert all timestamps to UTC during ingestion\nStore original timezone information in metadata\nUse ISO 8601 format (YYYY-MM-DDTHH:mm:ss.sssZ) as our standard\nMaintain source format for audit purposes\n\nCommon Format Handling\n\nSupport multiple input formats:\n\nUS format (MM/DD/YYYY)\nEuropean format (DD/MM/YYYY)\nUnix timestamps\nVarious timezone notations\nDate-only formats\nCustom ERP formats\n\n\nEdge Cases\n\nHandle invalid dates\nAccount for daylight savings transitions\nProcess missing timezone information\nManage partial dates (month/year only)\n\nQuality Controls\n\nValidate all conversions\nFlag impossible dates/times\nAlert on systematic conversion issues\nRegular audits of timestamp accuracy\n\n\n\n\n\n\n2.0.4 How will you maintain data lineage tracking across these diverse sources?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe implement a two-stage data lineage tracking system:\nRaw Data Stage (Pre-Iceberg)\n\nLanding Zone Tracking\n\nOriginal compressed files/CSVs logged in DynamoDB\nSource ERP system\nFile metadata (size, timestamp, checksum)\nProcessing status\nBatch/job ID\n\n\nTransformed Data Stage (In Iceberg)\n\nLeverage Iceberg’s features for:\n\nSnapshot history\nSchema evolution\nTime travel capabilities\nTransaction logs\n\n\nComplete Lineage Chain\n\nLink raw and transformed stages through:\n\nUnique batch IDs\nProcessing timestamps\nSource file references\nTransformation logs\n\n\nAudit Capabilities\n\nRaw data: Track original file to transformation\nTransformed data: Use Iceberg’s features\nEnd-to-end tracing from source file to final table\n\n\n\n\n\n\n\n3 Processing & Storage\n\n3.0.1 What’s your strategy for handling late-arriving data?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nScenarios:\n\nAn ERP system was down during the regular daily pull\nSomeone enters January sales data in March\nA business unit delays their data entry (enters last week’s data today)\nAn ERP has corrected/adjusted historical data\n\nOur strategy for handling late or backfilled data in daily batches:\nIdentification\n\nCompare record dates vs current processing date\nFlag any data older than previous day\nIdentify which historical partitions need updates\n\nProcessing\n\nStore data in Iceberg tables by business date, not ingestion date\nUse MERGE operations to handle updates to historical data\nKeep log of all backfilled data in DynamoDB for auditing\n\nMonitoring\n\nAlert if we see unusual amounts of historical data\nTrack which ERPs frequently send late data\nReport on data entry lag times\n\n\n\n\n\n\n3.0.2 How will you handle schema evolution in your Iceberg tables?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOur schema evolution strategy uses Iceberg’s built-in capabilities:\nSchema Changes\n\nAdd new columns as NULLABLE to maintain compatibility\nTrack all schema changes in version control\nDocument business reason for each change\nTest impact on downstream processes\n\nBackward Compatibility\n\nKeep previous schema versions accessible\nMaintain default values for new columns\nDocument field mappings for each version\nSupport queries across schema versions\n\nMigration Process\n\nRoll out schema changes in phases\nValidate data quality after changes\nKeep historical data accessible\nUpdate documentation and data dictionaries\n\nMonitoring & Control\n\nTrack which schema version each ERP uses\nAlert on unexpected schema changes\nRegular schema compatibility checks\nAudit schema change history\n\n\n\n\n\n\n3.0.3 What’s your plan for data validation before loading into S3?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nOur validation strategy leverages AWS Glue and PySpark capabilities:\nDrop Zone Validation (Initial S3 Landing)\n\nAWS Lambda performs quick checks:\n\nFile presence verification\nBasic file integrity\nNaming convention compliance\nFile size validation\nExpected file count per ERP\n\n\nPre-Iceberg Validation (Glue Job)\n\nPySpark validations during transformation:\n\nSchema conformance\nData type verification\nRequired field checks\nBusiness rule validation\nRecord count verification\nDate range completeness\n\n\nError Handling\n\nCloudWatch logs capture validation failures\nGlue job bookmarks track processed files\nGlue metrics monitor validation stats\nDynamoDB stores validation history\nFailed files moved to error prefix\nAlert notifications for validation failures\n\n\n\n\n\n\n3.0.4 How are you handling data versioning and rollbacks?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n4 Operational Concerns\n\n4.0.1 What’s your monitoring strategy for pipeline failures?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n4.0.2 How will you handle partial load failures?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n4.0.3 What’s your data retention policy in the drop zone?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n4.0.4 How will you manage pipeline dependencies between different ERP loads?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n\n5 Additional Questions\n\n5.0.1 What are the data sync frequency requirements for each system?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n5.0.2 Are there any specific SLAs for data freshness that need to be met?\n\n\n\n\n\n\nAnswer",
    "crumbs": [
      "Archive",
      "ETL Pipeline Planning Questions"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design.html",
    "href": "archive/erp-extractor-design.html",
    "title": "Design Document for Distributor Data Extraction Ingestion",
    "section": "",
    "text": "This design document covers the extraction ingestion component of our ETL pipeline. In our architecture, distributors integrate with our system via two primary methods:\n\nDirect Extraction Ingestion: Where our system automatically provisions the correct resources based solely on the three supplied parameters: --erp-type, --client-id, and --data-type.\nSFTP Ingestion: Where distributors SFTP their data to our environment (this method is handled separately).\n\nThe design detailed here pertains only to the direct extraction ingestion method. This process extracts distributor data and deposits it into a pre-dropzone S3 bucket. At this stage, a minor transformation is applied: column names are standardized according to our global data dictionary and the output files are converted to Parquet format. More complex transformations occur later in the pipeline.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design.html#overview",
    "href": "archive/erp-extractor-design.html#overview",
    "title": "Design Document for Distributor Data Extraction Ingestion",
    "section": "",
    "text": "This design document covers the extraction ingestion component of our ETL pipeline. In our architecture, distributors integrate with our system via two primary methods:\n\nDirect Extraction Ingestion: Where our system automatically provisions the correct resources based solely on the three supplied parameters: --erp-type, --client-id, and --data-type.\nSFTP Ingestion: Where distributors SFTP their data to our environment (this method is handled separately).\n\nThe design detailed here pertains only to the direct extraction ingestion method. This process extracts distributor data and deposits it into a pre-dropzone S3 bucket. At this stage, a minor transformation is applied: column names are standardized according to our global data dictionary and the output files are converted to Parquet format. More complex transformations occur later in the pipeline.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design.html#system-architecture",
    "href": "archive/erp-extractor-design.html#system-architecture",
    "title": "Design Document for Distributor Data Extraction Ingestion",
    "section": "2 System Architecture",
    "text": "2 System Architecture\n\n2.1 Entry Point and Application Hosting\n\nProgram Class:\n\nMain Method:\n\nParses command-line options (e.g., --erp-type, --client-id, --data-type) using System.CommandLine.\nSets up the dependency injection container and configures AWS services.\nResolves the primary extraction ingestion service (ERPService) from the DI container and triggers the extraction process.\n\nCreateHostBuilder Method:\n\nConfigures the host with AWS services, core providers, registries, and builder components.\nConfigures IHttpClientFactory with resilience policies for API-based integrations.\n\n\n\n\n\n2.2 Extraction Ingestion Orchestration\n\nERPService Class:\n\nActs as the orchestrator for the extraction ingestion process.\nRetrieves credentials and configuration from AWS Secrets Manager and DynamoDB, respectively.\nDynamically resolves components for data extraction based on ERP type.\nUses IHttpClientFactory for managing HTTP connections in API mode.\nSupports two extraction modes:\n\nAPI Mode: For ERPs that expose APIs.\nDatabase Mode: For ERPs that require direct database access.\n\nApplies a minimal transform:\n\nStandardizes column names according to our global data dictionary.\nConverts all files to Parquet format.\n\nDeposits the extracted (and minimally transformed) data into a pre-dropzone S3 bucket.\n(Note: Subsequent, more complex transformations are performed later in the overall ETL pipeline.)\n\n\n\n\n2.3 External Dependencies\n\nAWS Services:\n\nIAmazonSecretsManager: Retrieves ERP credentials.\nIAmazonDynamoDB: Fetches ERP configuration data.\nIAmazonS3: Uploads the extracted data into the pre-dropzone S3 bucket.\n\n.NET Libraries:\n\nSystem.CommandLine: For command-line parsing.\nMicrosoft.Extensions.Hosting & DI: For hosting and dependency injection.\nSystem.Text.Json: For JSON serialization/deserialization.\nIHttpClientFactory: For managing HttpClient instances in API extraction mode.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design.html#design-patterns-employed",
    "href": "archive/erp-extractor-design.html#design-patterns-employed",
    "title": "Design Document for Distributor Data Extraction Ingestion",
    "section": "3 Design Patterns Employed",
    "text": "3 Design Patterns Employed\n\n3.1 Dependency Injection (DI)\n\nUsage:\n\nDecouples service construction from business logic.\nRegisters AWS clients, providers, registries, builder components, and the main extraction service in the DI container.\nConfigures IHttpClientFactory and related services.\n\nBenefits:\n\nEnhances testability and maintainability.\nPromotes separation of concerns.\n\n\n\n\n3.2 Registry Pattern\n\nComponents:\n\nERPRegistry, ExtractorRegistry, TransformationRegistry, and UploaderRegistry.\n\nUsage:\n\nCentralizes lookup for ERP-specific factories and strategies.\nDynamically resolves connectors, extractors, transformers, and uploaders based on ERP type or data type.\n\nBenefits:\n\nSimplifies addition of new ERP integrations.\nReduces direct dependencies between the extraction process and concrete implementations.\n\n\n\n\n3.3 Abstract Factory Pattern\n\nComponent:\n\nThe IERPFactory interface (managed via ERPRegistry).\nIHttpClientFactory for HTTP client management.\n\nUsage:\n\nEncapsulates creation of ERP-specific components (e.g., connectors and jobs).\n\nBenefits:\n\nSupports multiple ERP systems with varying implementations without altering extraction logic.\n\n\n\n\n3.4 Builder Pattern\n\nComponents:\n\nAPIRequestBuilder & AuthenticationBuilder:\n\nProvide fluent interfaces to construct complex API request objects.\n\nDatabaseQueryBuilder (New):\n\nDynamically constructs SQL queries for ERPs requiring direct database access.\n\n\nUsage:\n\nThe DatabaseQueryBuilder collects parameters (e.g., ERP type, connection string, schema, table, etc.) and produces a DatabaseQuery object with a GenerateSql method.\nThe APIRequestBuilder works with IHttpClientFactory to construct properly configured HTTP requests.\n\nBenefits:\n\nEnhances readability and modularity.\nSupports both API and database extraction modes seamlessly.\n\n\n\n\n3.5 Strategy Pattern\n\nComponents:\n\nInterfaces such as IExtractor and ITransformer.\n\nUsage:\n\nEncapsulate different implementations for data extraction and minimal transformation.\nRegistries (like ExtractorRegistry and TransformationRegistry) select the appropriate strategy at runtime.\n\nBenefits:\n\nProvides flexibility to extend or change extraction and transformation algorithms without impacting the overall system.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design.html#detailed-component-descriptions",
    "href": "archive/erp-extractor-design.html#detailed-component-descriptions",
    "title": "Design Document for Distributor Data Extraction Ingestion",
    "section": "4 Detailed Component Descriptions",
    "text": "4 Detailed Component Descriptions\n\n4.1 Program Class\n\nMain Method:\n\nParses Nomad-supplied command-line arguments (only erp-type, client-id, and data-type are required).\nBuilds the DI container via CreateHostBuilder.\nResolves and invokes ERPService.ProcessERPData.\nHandles global errors for graceful failure.\n\nCreateHostBuilder Method:\n\nConfigures services including AWS clients, providers, registries, and builder components (for both API and database queries).\n\n\n\n\n4.2 ERPService Class\n\nResponsibilities:\n\nOrchestrates the extraction ingestion process.\nRetrieves credentials and ERP configuration.\nDynamically resolves ERP-specific components using registries.\nChooses between API or Database extraction modes based on the ERP configuration.\nApplies a minor transformation to standardize column names (per the global data dictionary) and converts files to Parquet.\nDeposits the minimally transformed data into a pre-dropzone S3 bucket.\n\nKey Method – ProcessERPData:\n\nCredential & Configuration Retrieval:\n\nUses ICredentialProvider and IConfigurationProvider to obtain ERP settings.\n\nDynamic Component Resolution:\n\nUses registries to resolve ERP-specific factories, extractors, transformers, and uploaders.\n\nIntegration Modes:\n\nAPI Mode:\n\nBuilds an API request via APIRequestBuilder (and AuthenticationBuilder) and extracts data via IExtractor.Extract.\n\nDatabase Mode:\n\nBuilds a SQL query using DatabaseQueryBuilder and extracts data via IExtractor.ExtractFromDatabase.\n\n\nSubsequent Steps:\n\nApplies the minor transform (standardizes column names and converts to Parquet).\nUploads the resulting data into a pre-dropzone S3 bucket via IDataUploader.\n\n\n\n\n\n4.3 Providers\n\nAWSCredentialProvider:\n\nRetrieves and deserializes credentials from AWS Secrets Manager.\n\nDynamoDBConfigProvider:\n\nFetches ERP configuration from DynamoDB and maps it to an ERPConfiguration object.\nNew Configuration Fields:\n\nAccessType: Indicates if the ERP uses API or Database.\nConnectionString, Schema, and BatchSize for database integrations.\n\n\n\n\n\n4.4 Data Uploader – S3DataUploader\n\nResponsibilities:\n\nFormats and uploads the minimally transformed data into S3.\nConverts files to Parquet format and ensures standardized column names.\n\nKey Methods:\n\nUpload: Manages the upload process.\nFormatData: Applies the transformation (e.g., renaming columns per the global data dictionary and converting to Parquet).\n\n\n\n\n4.5 Registries\n\nUploaderRegistry, ExtractorRegistry, TransformationRegistry, ERPRegistry:\n\nMaintain mappings from ERP type or data type to concrete implementations.\nProvide lookup methods (e.g., GetUploader, GetExtractor, GetStrategy, GetFactory) for dynamic resolution.\n\n\n\n\n4.6 Builder Components\n\n4.6.1 APIRequestBuilder & AuthenticationBuilder\n\nUsage:\n\nAllow fluent construction of API requests.\nSupport chaining methods to specify ERP type, endpoint, HTTP method, authentication, headers, query parameters, retry policy, and timeout.\n\n\n\n\n4.6.2 DatabaseQueryBuilder\n\nResponsibilities:\n\nProvides a fluent interface for building SQL queries for ERP systems that require direct database access.\n\nChainable Methods:\n\nForERP, WithConnectionString, WithSchema, WithTable, WithColumns, WithWhere, WithOrderBy, WithLimit, WithOffset, WithParameter, WithCommandTimeout, WithIsolationLevel\n\nBuild Method:\n\nValidates required parameters and constructs a DatabaseQuery object.\n\nDatabaseQuery Object:\n\nContains properties for ERP type, connection string, schema, table, columns, conditions, ordering, limits, parameters, command timeout, and isolation level.\nProvides a GenerateSql method to convert query parameters into a valid SQL string.\n\n\n\n\n\n4.7 Configuration Models\n\nERPConfiguration:\n\nStores settings such as BaseUrl, CompanyId, WarehouseId, RequiredHeaders, and timeout/retry settings.\nNew Fields for Database Access:\n\nAccessType: Enum (API or Database).\nConnectionString: For direct database connections.\nSchema: Database schema.\nBatchSize: Number of records to fetch per batch.\n\n\nERPCredentials:\n\nHolds secure API keys and client secrets.\n\nUploadConfiguration:\n\nUsed by the data uploader to configure the S3 upload.\n\nAccessType Enum:\n\nDistinguishes between API and Database access modes.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design.html#pseudo-code-for-nomad-integration",
    "href": "archive/erp-extractor-design.html#pseudo-code-for-nomad-integration",
    "title": "Design Document for Distributor Data Extraction Ingestion",
    "section": "5 Pseudo-code for Nomad Integration",
    "text": "5 Pseudo-code for Nomad Integration\n/// &lt;summary&gt;\n/// Entry point for the ERP data extraction and ingestion process.\n/// Processes command-line arguments from Nomad and orchestrates the ETL workflow.\n/// &lt;/summary&gt;\nMain:\n    // Parse Nomad-supplied command-line arguments:\n    //   --erp-type, --client-id, --data-type\n    options = parseArguments([\"--erp-type\", \"--client-id\", \"--data-type\"])\n\n    // Build the host container with dependency injection configured\n    host = createHostBuilder().build()\n\n    // Retrieve the ERPService from the DI container\n    erpService = host.getService(ERPService)\n\n    // Trigger the extraction ingestion process with the supplied parameters\n    erpService.ProcessERPData(options.erpType, options.clientId, options.dataType)\n\n/// &lt;summary&gt;\n/// Processes ERP data extraction and performs initial transformation.\n/// &lt;/summary&gt;\n/// &lt;param name=\"erpType\"&gt;The type of ERP system to extract from&lt;/param&gt;\n/// &lt;param name=\"clientId\"&gt;The client identifier&lt;/param&gt;\n/// &lt;param name=\"dataType\"&gt;The type of data to extract&lt;/param&gt;\n/// &lt;remarks&gt;\n/// This method handles both API and Database extraction modes. For API mode,\n/// it constructs appropriate API requests with authentication. For Database mode,\n/// it builds and executes SQL queries. In both cases, the extracted data is:\n/// 1. Minimally transformed (column standardization)\n/// 2. Converted to Parquet format\n/// 3. Uploaded to a pre-dropzone S3 bucket\n/// &lt;/remarks&gt;\nERPService.ProcessERPData(erpType, clientId, dataType):\n    Log \"Starting ETL extraction ingestion for client [clientId] using ERP [erpType]\"\n\n    /// &lt;summary&gt;Retrieve credentials from AWS Secrets Manager&lt;/summary&gt;\n    credentials = CredentialProvider.GetCredentials(erpType, clientId)\n\n    /// &lt;summary&gt;Retrieve configuration from DynamoDB&lt;/summary&gt;\n    erpConfig = ConfigurationProvider.GetConfiguration(erpType, clientId)\n\n    /// &lt;summary&gt;\n    /// Lookup common components via registries:\n    /// - ERP-specific factory (for connectors and jobs)\n    /// - Data extractor (for API or DB extraction)\n    /// - Data transformer (to standardize columns and convert to Parquet)\n    /// - Data uploader (to upload data to the pre-dropzone S3 bucket)\n    /// &lt;/summary&gt;\n    factory = ERPRegistry.GetFactory(erpType, clientId)\n    extractor = ExtractorRegistry.GetExtractor(erpType)\n    transformer = TransformationRegistry.GetStrategy(erpType, dataType)\n    uploader = UploaderRegistry.GetUploader(\"s3\")\n\n    /// &lt;summary&gt;\n    /// Handle database extraction mode\n    /// Builds and executes SQL queries for direct database access\n    /// &lt;/summary&gt;\n    if erpConfig.AccessType == Database then:\n        query = DatabaseQueryBuilder()\n                  .ForERP(erpType)\n                  .WithConnectionString(erpConfig.ConnectionString)\n                  .WithSchema(erpConfig.Schema)\n                  .WithTable(dataType + \"_table\")\n                  .WithColumns(\"id\", \"created_at\", \"data\")\n                  .WithWhere(\"is_processed\", false)\n                  .WithOrderBy(\"created_at\")\n                  .WithLimit(erpConfig.BatchSize)\n                  .WithCommandTimeout(erpConfig.TimeoutSeconds)\n                  .Build()\n        \n        Log \"Executing database query: \" + query.GenerateSql()\n        extractedData = extractor.ExtractFromDatabase(query)\n\n    /// &lt;summary&gt;\n    /// Handle API extraction mode\n    /// Constructs and executes authenticated API requests\n    /// &lt;/summary&gt;\n    else:\n        request = APIRequestBuilder()\n                    .ForERP(erpType)\n                    .WithEndpoint(erpConfig.BaseUrl + \"/api/v2/sales\")\n                    .WithMethod(GET)\n                    .WithAuthentication(\n                        AuthenticationBuilder()\n                            .WithApiKey(credentials.ApiKey)\n                            .WithClientId(credentials.ClientId)\n                            .WithClientSecret(credentials.ClientSecret)\n                            .Build()\n                    )\n                    .WithHeaders(erpConfig.RequiredHeaders)\n                    .WithQueryParameters({\n                        \"companyId\": erpConfig.CompanyId,\n                        \"warehouse\": erpConfig.WarehouseId,\n                        \"pageSize\": erpConfig.PageSize.toString()\n                    })\n                    .WithRetryPolicy(erpConfig.MaxRetries)\n                    .WithTimeout(erpConfig.TimeoutSeconds)\n                    .Build()\n        \n        Log \"Executing API request to \" + erpConfig.BaseUrl + \"/api/v2/sales\"\n        extractedData = extractor.Extract(request)\n\n    /// &lt;summary&gt;Transform the extracted data (standardize columns and convert to Parquet)&lt;/summary&gt;\n    Log \"Starting minor data transformation\"\n    transformedData = transformer.Transform(extractedData)\n    \n    /// &lt;summary&gt;\n    /// Configure and execute the S3 upload operation\n    /// Data is stored in a pre-dropzone bucket with standardized path structure\n    /// &lt;/summary&gt;\n    uploadConfig = new UploadConfiguration(\n                    Bucket: \"erp-data-\" + clientId,\n                    Key: erpType + \"/\" + dataType + \"/\" + currentTimestamp + \"/data.parquet\",\n                    Format: Parquet,\n                    Metadata: {\n                       \"erp_type\": erpType,\n                       \"client_id\": clientId,\n                       \"data_type\": dataType,\n                       \"extract_timestamp\": currentTimestamp\n                    }\n                )\n    \n    Log \"Starting data upload to S3 pre-dropzone\"\n    uploader.Upload(transformedData, uploadConfig)\n\n    Log \"Extraction ingestion process completed successfully\"",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design.html#error-handling-and-logging",
    "href": "archive/erp-extractor-design.html#error-handling-and-logging",
    "title": "Design Document for Distributor Data Extraction Ingestion",
    "section": "6 Error Handling and Logging",
    "text": "6 Error Handling and Logging\n\nError Handling:\n\nTry-catch blocks around critical operations (AWS calls, extraction, and query building).\nValidations in builder components ensure required fields are provided.\n\nLogging:\n\nILogger&lt;T&gt; is used to log key events and errors.\nHTTP request/response logging through IHttpClientFactory.\nDetailed logs enable tracing of the extraction ingestion workflow.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design.html#external-dependencies-and-integration",
    "href": "archive/erp-extractor-design.html#external-dependencies-and-integration",
    "title": "Design Document for Distributor Data Extraction Ingestion",
    "section": "7 External Dependencies and Integration",
    "text": "7 External Dependencies and Integration\n\nAWS Services:\n\nSecrets Manager: Securely retrieves credentials.\nDynamoDB: Provides ERP configuration data.\nS3: Stores the minimally transformed data in the pre-dropzone.\n\nDatabase Integration:\n\nFor ERP systems without APIs, direct database queries are supported using DatabaseQueryBuilder and DatabaseQuery.\n\nHTTP Integration:\n\nManaged through IHttpClientFactory for API-based ERPs.\nImplements resilience patterns via Polly policies.\nCentralizes HTTP client configuration and lifecycle management.\n\n.NET Libraries:\n\nSystem.CommandLine: For CLI parsing.\nMicrosoft.Extensions.Hosting/DI: For application hosting and dependency injection.\nJsonSerializer: For data serialization tasks.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/erp-extractor-design.html#conclusion",
    "href": "archive/erp-extractor-design.html#conclusion",
    "title": "Design Document for Distributor Data Extraction Ingestion",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nThe extraction ingestion component of the ETL pipeline for distributor data automatically provisions the correct resources based solely on three Nomad-supplied parameters (erp-type, client-id, and data-type). It supports both API and database extraction modes and applies a minor transformation—standardizing column names and converting files to Parquet—before depositing the data into a pre-dropzone S3 bucket. More complex transformations occur later in the overall pipeline, and a separate ingestion method is provided for distributors that SFTP their data. The architecture and design patterns employed enable a flexible, maintainable, and scalable solution.",
    "crumbs": [
      "Archive",
      "Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/ml.html",
    "href": "archive/ml.html",
    "title": "Pet Product Classification System Documentation",
    "section": "",
    "text": "This project implements an automated product classification system that uses embedding-based similarity search and Large Language Models (LLMs) to categorize product descriptions. The system combines RAG (Retrieval Augmented Generation) techniques with both local Llama3.1 and gpt4 language models to provide structured classification outputs.\n\n\n\nPull distributor description from csv file (csv file was create by querying from md.Sales joined with md.Items)\nFind similar master descriptions (from csv created from md.Items) then pull those with needed categories (species, type, lifestage, etc.)\nCreate a prompt for LLM using distributor description and master descriptions with needed categories\n\nLooks prompt looks something like:\n\nWe want you to classify a new pet product into these categories: (species, type, lifestage, etc.). Here is a new description: {new_description} and here are some previous classifications to use as reference: {list of master descriptions with categories}. Here are some common abbreviation to keep in mind {list of abbreviations}.\n\n\nProcess LLM response\nWrite results to a csv file\n\n\n\n\n\nConfig Management (Config)\n\nCentralizes all configuration settings including model parameters, file paths, and required categories\nHandles both local (ollama) and cloud (OpenAI) model configurations\n\nData Processing Pipeline\n\nText Preprocessing (PreprocessText)\n\nCleans and standardizes input text\nGenerates embeddings for product descriptions\n\nEmbedding Search (EmbeddingSearcher)\n\nUses FAISS for efficient similarity search for find similar master descriptions\nMaintains an index of product description embeddings\nRetrieves similar products for RAG\n\nFormatting (Formatter)\n\nHandles data formatting for model inputs\nProcesses model outputs\nRecords results\n\n\nCore Processing (Process)\n\nOrchestrates the entire classification workflow\nManages model interactions\nHandles both batch and single-item processing\n\nOutput Structure (StructuredOutput)\n\nDefines the schema for classification outputs\nIncludes fields like Type, Formula, Species, etc.\nUses Pydantic for data validation\n\n\n\n\n\n\nSupports both GPT and Llama models\nRAG-based classification using similar product examples\nEfficient vector similarity search using FAISS\nBatch processing capabilities\nStructured output validation\nPerformance monitoring and logging\nError handling with Result type pattern\n\n\n\n\n\n\nInput Processing\n\nClean and standardize product descriptions\nGenerate embeddings using SentenceTransformer\n\nSimilar Product Retrieval\n\nSearch for similar master products using FAISS\nRetrieve top-k similar products as examples\n\nClassification\n\nFormat prompt with new description and similar examples\nSend to selected LLM (Llama or GPT)\nParse and validate structured response\n\nOutput Processing\n\nRecord results and performance metrics\nStore in specified output format\n\n\n\n\n\n\nDependencies\n\nOpenAI API (for GPT models)\nSentenceTransformer\nFAISS\nPandas\nPydantic\nRequests (for Llama API)\n\nConfiguration\n\nModel selection (Llama or GPT)\nAPI keys and endpoints\nEmbedding model parameters\nFile paths for data and outputs\n\n\n\n\n\n\n\n# Initialize processor\nprocess = LLMProcess(config)\n\n# num_examples defines how many master description examples we give the LLMs as reference data\n# Classify single description\nresult = process.classify_description(new_description, num_examples=5)\n\n\n\n# new_description_df -&gt; Pandas dataframe with many new descriptions\n# description_col -&gt; the column in dataframe where descriptions are \ndescription_col = 'ItemDescription'\n\n# Process multiple descriptions\nresults = process.classify_batch(new_descriptions_df, description_col, num_examples=50)"
  },
  {
    "objectID": "archive/ml.html#overview",
    "href": "archive/ml.html#overview",
    "title": "Pet Product Classification System Documentation",
    "section": "",
    "text": "This project implements an automated product classification system that uses embedding-based similarity search and Large Language Models (LLMs) to categorize product descriptions. The system combines RAG (Retrieval Augmented Generation) techniques with both local Llama3.1 and gpt4 language models to provide structured classification outputs.\n\n\n\nPull distributor description from csv file (csv file was create by querying from md.Sales joined with md.Items)\nFind similar master descriptions (from csv created from md.Items) then pull those with needed categories (species, type, lifestage, etc.)\nCreate a prompt for LLM using distributor description and master descriptions with needed categories\n\nLooks prompt looks something like:\n\nWe want you to classify a new pet product into these categories: (species, type, lifestage, etc.). Here is a new description: {new_description} and here are some previous classifications to use as reference: {list of master descriptions with categories}. Here are some common abbreviation to keep in mind {list of abbreviations}.\n\n\nProcess LLM response\nWrite results to a csv file\n\n\n\n\n\nConfig Management (Config)\n\nCentralizes all configuration settings including model parameters, file paths, and required categories\nHandles both local (ollama) and cloud (OpenAI) model configurations\n\nData Processing Pipeline\n\nText Preprocessing (PreprocessText)\n\nCleans and standardizes input text\nGenerates embeddings for product descriptions\n\nEmbedding Search (EmbeddingSearcher)\n\nUses FAISS for efficient similarity search for find similar master descriptions\nMaintains an index of product description embeddings\nRetrieves similar products for RAG\n\nFormatting (Formatter)\n\nHandles data formatting for model inputs\nProcesses model outputs\nRecords results\n\n\nCore Processing (Process)\n\nOrchestrates the entire classification workflow\nManages model interactions\nHandles both batch and single-item processing\n\nOutput Structure (StructuredOutput)\n\nDefines the schema for classification outputs\nIncludes fields like Type, Formula, Species, etc.\nUses Pydantic for data validation\n\n\n\n\n\n\nSupports both GPT and Llama models\nRAG-based classification using similar product examples\nEfficient vector similarity search using FAISS\nBatch processing capabilities\nStructured output validation\nPerformance monitoring and logging\nError handling with Result type pattern"
  },
  {
    "objectID": "archive/ml.html#workflow",
    "href": "archive/ml.html#workflow",
    "title": "Pet Product Classification System Documentation",
    "section": "",
    "text": "Input Processing\n\nClean and standardize product descriptions\nGenerate embeddings using SentenceTransformer\n\nSimilar Product Retrieval\n\nSearch for similar master products using FAISS\nRetrieve top-k similar products as examples\n\nClassification\n\nFormat prompt with new description and similar examples\nSend to selected LLM (Llama or GPT)\nParse and validate structured response\n\nOutput Processing\n\nRecord results and performance metrics\nStore in specified output format"
  },
  {
    "objectID": "archive/ml.html#setup-requirements",
    "href": "archive/ml.html#setup-requirements",
    "title": "Pet Product Classification System Documentation",
    "section": "",
    "text": "Dependencies\n\nOpenAI API (for GPT models)\nSentenceTransformer\nFAISS\nPandas\nPydantic\nRequests (for Llama API)\n\nConfiguration\n\nModel selection (Llama or GPT)\nAPI keys and endpoints\nEmbedding model parameters\nFile paths for data and outputs"
  },
  {
    "objectID": "archive/ml.html#usage-examples",
    "href": "archive/ml.html#usage-examples",
    "title": "Pet Product Classification System Documentation",
    "section": "",
    "text": "# Initialize processor\nprocess = LLMProcess(config)\n\n# num_examples defines how many master description examples we give the LLMs as reference data\n# Classify single description\nresult = process.classify_description(new_description, num_examples=5)\n\n\n\n# new_description_df -&gt; Pandas dataframe with many new descriptions\n# description_col -&gt; the column in dataframe where descriptions are \ndescription_col = 'ItemDescription'\n\n# Process multiple descriptions\nresults = process.classify_batch(new_descriptions_df, description_col, num_examples=50)"
  },
  {
    "objectID": "archive/enhanced-erp-extractor-design.html",
    "href": "archive/enhanced-erp-extractor-design.html",
    "title": "Enhanced Design Document for Distributor Data Extraction Ingestion",
    "section": "",
    "text": "This design document covers the extraction ingestion component of our ETL pipeline. In our architecture, distributors integrate with our system via two primary methods:\n\nDirect Extraction Ingestion: Where our system automatically provisions the correct resources based solely on the three supplied parameters: --erp-type, --client-id, and --data-type.\nSFTP Ingestion: Where distributors SFTP their data to our environment (this method is handled separately).\n\nThe design detailed here pertains only to the direct extraction ingestion method. This process extracts distributor data and deposits it into a pre-dropzone S3 bucket. At this stage, a minor transformation is applied: column names are standardized according to our global data dictionary and the output files are converted to Parquet format. More complex transformations occur later in the pipeline.",
    "crumbs": [
      "Archive",
      "Enhanced Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/enhanced-erp-extractor-design.html#overview",
    "href": "archive/enhanced-erp-extractor-design.html#overview",
    "title": "Enhanced Design Document for Distributor Data Extraction Ingestion",
    "section": "",
    "text": "This design document covers the extraction ingestion component of our ETL pipeline. In our architecture, distributors integrate with our system via two primary methods:\n\nDirect Extraction Ingestion: Where our system automatically provisions the correct resources based solely on the three supplied parameters: --erp-type, --client-id, and --data-type.\nSFTP Ingestion: Where distributors SFTP their data to our environment (this method is handled separately).\n\nThe design detailed here pertains only to the direct extraction ingestion method. This process extracts distributor data and deposits it into a pre-dropzone S3 bucket. At this stage, a minor transformation is applied: column names are standardized according to our global data dictionary and the output files are converted to Parquet format. More complex transformations occur later in the pipeline.",
    "crumbs": [
      "Archive",
      "Enhanced Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/enhanced-erp-extractor-design.html#system-architecture",
    "href": "archive/enhanced-erp-extractor-design.html#system-architecture",
    "title": "Enhanced Design Document for Distributor Data Extraction Ingestion",
    "section": "2 System Architecture",
    "text": "2 System Architecture\n\n2.1 Entry Point and Application Hosting\n\nProgram Class:\n\nMain Method:\n\nParses command-line options (e.g., --erp-type, --client-id, --data-type) using System.CommandLine.\nSets up the dependency injection container and configures services.\nResolves the primary extraction ingestion service (ERPService) from the DI container and triggers the extraction process.\n\nCreateHostBuilder Method:\n\nConfigures the host with HashiCorp Vault, FerretDB services, core providers, registries, and builder components.\nConfigures IHttpClientFactory with resilience policies for API-based integrations.\nSets up feature flags, metrics collection, and health checks.\n\n\n\n\n\n2.2 Extraction Ingestion Orchestration\n\nERPService Class:\n\nActs as the orchestrator for the extraction ingestion process.\nRetrieves credentials from HashiCorp Vault and configuration from FerretDB, respectively.\nDynamically resolves components for data extraction based on ERP type.\nUses IHttpClientFactory for managing HTTP connections in API mode.\nSupports two extraction modes:\n\nAPI Mode: For ERPs that expose APIs.\nDatabase Mode: For ERPs that require direct database access.\n\nSupports both batch processing and incremental extraction with change data capture (CDC).\nApplies a minimal transform:\n\nStandardizes column names according to our global data dictionary.\nApplies data quality validations against predefined rules.\nConverts all files to Parquet format with optional compression.\n\nDeposits the extracted (and minimally transformed) data into a pre-dropzone S3 bucket.\nTracks data lineage for audit and compliance purposes.\n(Note: Subsequent, more complex transformations are performed later in the overall ETL pipeline.)\n\n\n\n\n2.3 External Dependencies\n\nInfrastructure Services:\n\nIVaultService: Retrieves ERP credentials from HashiCorp Vault with least privilege access.\nIFerretDBService: Fetches ERP configuration data from FerretDB.\nIAmazonS3: Uploads the extracted data into the pre-dropzone S3 bucket.\nIFeatureFlagService: Manages feature flags for gradual rollout of functionality.\nICatalogService: Integrates with data catalog for metadata management.\n\n.NET Libraries:\n\nSystem.CommandLine: For command-line parsing.\nMicrosoft.Extensions.Hosting & DI: For hosting and dependency injection.\nSystem.Text.Json: For JSON serialization/deserialization.\nIHttpClientFactory: For managing HttpClient instances in API extraction mode.\nVaultSharp: For interacting with HashiCorp Vault.\nMongoDB.Driver: For FerretDB interactions (FerretDB uses MongoDB wire protocol).\nPolly: For resilience patterns including circuit breakers and bulkheads.\nOpenTelemetry: For distributed tracing and metrics.\nFluentValidation: For data contract validation.",
    "crumbs": [
      "Archive",
      "Enhanced Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/enhanced-erp-extractor-design.html#design-patterns-and-advanced-resilience-strategies",
    "href": "archive/enhanced-erp-extractor-design.html#design-patterns-and-advanced-resilience-strategies",
    "title": "Enhanced Design Document for Distributor Data Extraction Ingestion",
    "section": "3 Design Patterns and Advanced Resilience Strategies",
    "text": "3 Design Patterns and Advanced Resilience Strategies\n\n3.1 Dependency Injection (DI)\n\nUsage:\n\nDecouples service construction from business logic.\nRegisters Vault client, FerretDB connection, providers, registries, builder components, and the main extraction service in the DI container.\nConfigures IHttpClientFactory and related services.\n\nBenefits:\n\nEnhances testability and maintainability.\nPromotes separation of concerns.\n\n\n\n\n3.2 Registry Pattern\n\nComponents:\n\nERPRegistry, ExtractorRegistry, TransformationRegistry, and UploaderRegistry.\n\nUsage:\n\nCentralizes lookup for ERP-specific factories and strategies.\nDynamically resolves connectors, extractors, transformers, and uploaders based on ERP type or data type.\n\nBenefits:\n\nSimplifies addition of new ERP integrations.\nReduces direct dependencies between the extraction process and concrete implementations.\n\n\n\n\n3.3 Abstract Factory Pattern\n\nComponent:\n\nThe IERPFactory interface (managed via ERPRegistry).\nIHttpClientFactory for HTTP client management.\n\nUsage:\n\nEncapsulates creation of ERP-specific components (e.g., connectors and jobs).\n\nBenefits:\n\nSupports multiple ERP systems with varying implementations without altering extraction logic.\n\n\n\n\n3.4 Builder Pattern\n\nComponents:\n\nAPIRequestBuilder & AuthenticationBuilder:\n\nProvide fluent interfaces to construct complex API request objects.\n\nDatabaseQueryBuilder:\n\nDynamically constructs SQL queries for ERPs requiring direct database access.\n\nExtractConfigBuilder:\n\nBuilds extraction configurations supporting both full and incremental extracts.\n\n\nUsage:\n\nThe DatabaseQueryBuilder collects parameters and produces a DatabaseQuery object with a GenerateSql method.\nThe APIRequestBuilder works with IHttpClientFactory to construct properly configured HTTP requests.\nThe ExtractConfigBuilder creates configurations for different extraction modes.\n\nBenefits:\n\nEnhances readability and modularity.\nSupports multiple extraction modes seamlessly.\n\n\n\n\n3.5 Strategy Pattern\n\nComponents:\n\nInterfaces such as IExtractor, ITransformer, and IValidator.\n\nUsage:\n\nEncapsulate different implementations for data extraction, transformation, and validation.\nRegistries select the appropriate strategy at runtime.\n\nBenefits:\n\nProvides flexibility to extend or change algorithms without impacting the overall system.\n\n\n\n\n3.6 Decorator Pattern\n\nComponents:\n\nVaultCredentialProviderDecorator:\n\nAdds caching behavior to Vault credential retrieval.\n\nMetricsDecorator:\n\nWraps core services to collect performance metrics.\n\nEncryptionDecorator:\n\nAdds field-level encryption for sensitive data.\n\nDataQualityDecorator:\n\nAdds data quality checks to transformations.\n\n\nUsage:\n\nEnhances existing components with additional functionality without modifying their core logic.\n\nBenefits:\n\nImproves performance through caching.\nProvides observability and enhances security.\nEnsures data quality through validation.\n\n\n\n\n3.7 Circuit Breaker Pattern\n\nComponents:\n\nVaultCircuitBreaker, FerretDBCircuitBreaker, S3CircuitBreaker\n\nUsage:\n\nPrevents cascading failures by breaking the circuit when dependencies fail.\nAutomatically restores service when dependencies recover.\n\nBenefits:\n\nEnhances system resilience during partial outages.\nPrevents overwhelming failing services with requests.\n\n\n\n\n3.8 Bulkhead Pattern\n\nComponents:\n\nSeparate connection pools for different ERP types and operations.\nIsolated resource pools for critical vs. non-critical operations.\n\nUsage:\n\nIsolates failures to prevent system-wide degradation.\nAllocates resources based on operation criticality.\n\nBenefits:\n\nPrevents resource exhaustion.\nImproves system stability during partial failures.\n\n\n\n\n3.9 Exponential Backoff with Jitter\n\nUsage:\n\nImplements intelligent retry strategies for all external service calls.\nAdds randomization to prevent thundering herd problems.\n\nBenefits:\n\nPrevents overwhelming recovering services.\nDistributes retry attempts evenly over time.",
    "crumbs": [
      "Archive",
      "Enhanced Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/enhanced-erp-extractor-design.html#detailed-component-descriptions",
    "href": "archive/enhanced-erp-extractor-design.html#detailed-component-descriptions",
    "title": "Enhanced Design Document for Distributor Data Extraction Ingestion",
    "section": "4 Detailed Component Descriptions",
    "text": "4 Detailed Component Descriptions\n\n4.1 Program Class\n\nMain Method:\n\nParses Nomad-supplied command-line arguments.\nBuilds the DI container via CreateHostBuilder.\nResolves and invokes ERPService.ProcessERPData.\nHandles global errors for graceful failure.\n\nCreateHostBuilder Method:\n\nConfigures services including Vault client, FerretDB connection, providers, registries, and builder components.\nSets up resilience policies with circuit breakers and bulkheads.\nConfigures feature flags, metrics collection, and health checks.\nSets up OpenAPI documentation generation.\n\n\n\n\n4.2 ERPService Class\n\nResponsibilities:\n\nOrchestrates the extraction ingestion process.\nRetrieves credentials and ERP configuration.\nDynamically resolves ERP-specific components using registries.\nChooses between API or Database extraction modes based on the ERP configuration.\nSupports both batch and incremental extraction modes.\nApplies a minor transformation with data quality validation.\nDeposits the transformed data into a pre-dropzone S3 bucket.\nTracks data lineage for audit and compliance.\n\nKey Method – ProcessERPData:\n\nCredential & Configuration Retrieval:\n\nUses ICredentialProvider to obtain short-lived, least-privilege credentials.\nUses IConfigurationProvider to obtain ERP settings.\n\nDynamic Component Resolution:\n\nUses registries to resolve ERP-specific factories, extractors, transformers, and uploaders.\n\nIntegration Modes:\n\nAPI Mode:\n\nBuilds an API request via APIRequestBuilder with mutual TLS if supported.\nExtracts data via IExtractor.Extract with circuit breaker protection.\n\nDatabase Mode:\n\nBuilds a SQL query using DatabaseQueryBuilder.\nExtracts data via IExtractor.ExtractFromDatabase with connection isolation.\n\n\nExtraction Modes:\n\nFull Extract: Retrieves all data for the specified type.\nIncremental Extract: Retrieves only changed data since the last extraction.\n\nSelf-Healing:\n\nImplements automatic recovery procedures for common failure scenarios.\n\nSubsequent Steps:\n\nApplies the minor transform with data quality validation.\nRecords data lineage metadata.\nUploads the resulting data into a pre-dropzone S3 bucket via IDataUploader.\n\n\n\n\n\n4.3 Providers\n\nVaultCredentialProvider:\n\nRetrieves and deserializes credentials from HashiCorp Vault.\nImplements caching with TTL-based invalidation.\nSupports dynamic secret rotation with configurable TTL.\nGenerates least-privilege, operation-specific credentials.\n\nFerretDBConfigProvider:\n\nFetches ERP configuration from FerretDB and maps it to an ERPConfiguration object.\nUses MongoDB driver since FerretDB implements MongoDB wire protocol.\nSupports read-preference strategies for replica sets.\nImplements schema evolution for backward compatibility.\n\n\n\n\n4.4 Data Uploader – S3DataUploader\n\nResponsibilities:\n\nFormats and uploads the minimally transformed data into S3.\nApplies data compression for efficient storage and transfer.\nConverts files to Parquet format and ensures standardized column names.\nEncrypts sensitive fields before upload.\nRecords data lineage metadata.\n\nKey Methods:\n\nUpload: Manages the upload process with checksums and integrity verification.\nFormatData: Applies the transformation with data quality checks.\nTrackLineage: Records data provenance information.\n\n\n\n\n4.5 Data Quality and Validation\n\nDataContractValidator:\n\nResponsibilities:\n\nValidates data against predefined schemas and rules.\nReports quality issues with detailed diagnostics.\nEnforces data governance policies.\n\nKey Methods:\n\nValidateSchema: Ensures data adheres to expected structure.\nValidateValues: Checks data values against business rules.\nGenerateReport: Creates detailed validation reports.\n\n\nDataMaskingService:\n\nResponsibilities:\n\nIdentifies and masks sensitive information (PII).\nSupports various masking techniques (hashing, tokenization, etc.).\nMaintains referential integrity across masked datasets.\n\nKey Methods:\n\nIdentifySensitiveFields: Automatically detects potential PII.\nApplyMasking: Applies appropriate masking techniques.\nVerifyMasking: Ensures masking effectiveness.\n\n\n\n\n\n4.6 HashiCorp Vault Integration\n\nVaultService:\n\nResponsibilities:\n\nManages connection to HashiCorp Vault with circuit breaker protection.\nRetrieves secrets with proper authentication.\nHandles secret versioning and rotation.\nGenerates dynamic, least-privilege credentials.\n\nKey Methods:\n\nGetSecret: Retrieves a secret by path.\nGetDynamicSecret: Retrieves a dynamic secret with lease management.\nRenewToken: Handles token renewal to maintain access.\n\nConfiguration:\n\nSupport for multiple authentication methods (AppRole, Token, K8s).\nAutomatic token renewal.\nSecret caching with TTL-based invalidation.\nMutual TLS for secure communication.\n\n\nVaultConfiguration:\n\nStores settings such as Address, AuthMethod, RoleId, SecretId, and authentication paths.\nIncludes retry and timeout settings with exponential backoff and jitter.\n\n\n\n\n4.7 FerretDB Integration\n\nFerretDBService:\n\nResponsibilities:\n\nManages connection to FerretDB using MongoDB driver.\nExecutes queries and maps results to domain models.\nHandles connection pooling and resilience.\nSupports schema evolution for backward compatibility.\n\nKey Methods:\n\nGetConfiguration: Retrieves configuration by ERP type and client ID.\nUpdateConfiguration: Updates configuration documents.\nExecuteQuery: Executes a MongoDB query and returns the results.\nGetSchemaVersion: Retrieves schema version information.\n\nConfiguration:\n\nSupport for replica sets and read preferences.\nConnection pooling with bulkhead isolation.\nAutomatic retry with exponential backoff and jitter.\nCircuit breaker protection against cascading failures.\n\n\nFerretDBConfiguration:\n\nStores settings such as ConnectionString, Database, Collection, and authentication credentials.\nIncludes timeout and retry settings.\nDefines bulkhead configuration for connection isolation.\n\n\n\n\n4.8 Feature Flag Management\n\nFeatureFlagService:\n\nResponsibilities:\n\nManages feature flags for gradual rollout of new features.\nSupports A/B testing and canary releases.\nProvides runtime configuration without deployment.\n\nKey Methods:\n\nIsFeatureEnabled: Checks if a feature is enabled for a specific context.\nGetFeatureConfiguration: Retrieves configuration for an enabled feature.\nRecordFeatureUsage: Tracks feature usage for analytics.\n\nConfiguration:\n\nSupport for user, client, and global targeting.\nTime-based and percentage-based rollouts.\nIntegration with monitoring for impact assessment.\n\n\n\n\n\n4.9 Metrics and Observability\n\nMetricsService:\n\nCollects performance metrics on key operations.\nIntegrates with Prometheus for metrics collection.\nSupports custom dimensions for detailed analysis.\nRecords histogram metrics for latency distribution.\n\nHealthCheckService:\n\nProvides health status of dependencies (Vault, FerretDB, S3).\nImplements circuit breaker pattern for degraded services.\nExposes health endpoints for monitoring.\nSupports self-healing procedures for common issues.\n\nTracingService:\n\nProvides distributed tracing across system components.\nIntegrates with OpenTelemetry for standardized telemetry.\nCorrelates logs, metrics, and traces for holistic observability.\nSupports sampling strategies for high-volume production environments.\n\n\n\n\n4.10 Data Lineage and Governance\n\nDataLineageService:\n\nResponsibilities:\n\nTracks data origin, transformations, and destinations.\nRecords metadata about extraction processes.\nSupports audit requirements and compliance verification.\n\nKey Methods:\n\nStartLineageRecord: Creates a new lineage tracking record.\nRecordTransformation: Logs applied transformations.\nCompleteLineageRecord: Finalizes the lineage record.\n\nIntegration:\n\nConnects with data catalog for metadata management.\nProvides lineage visualization through API.\nSupports data governance and compliance reporting.\n\n\n\n\n\n4.11 Configuration Models\n\nERPConfiguration:\n\nStores settings such as BaseUrl, CompanyId, WarehouseId, RequiredHeaders, and timeout/retry settings.\nFields for Database Access:\n\nAccessType: Enum (API or Database).\nConnectionString: For direct database connections.\nSchema: Database schema.\nBatchSize: Number of records to fetch per batch.\n\nFields for Incremental Extract:\n\nSupportsCDC: Whether the ERP supports Change Data Capture.\nCDCConfiguation: Settings for CDC-based extraction.\nWatermarkColumn: Column used for incremental extraction.\n\n\nERPCredentials:\n\nHolds secure API keys and client secrets.\nIncludes operation-specific scoped credentials.\nSupports dynamic credential generation.\n\nUploadConfiguration:\n\nUsed by the data uploader to configure the S3 upload.\nIncludes compression settings and encryption options.\nDefines metadata for data catalog integration.\n\nResilienceConfiguration:\n\nDefines circuit breaker thresholds and recovery periods.\nConfigures retry policies with exponential backoff and jitter.\nSpecifies bulkhead isolation settings for resource pools.\n\n\n\n\n4.12 Development and Test Support\n\nLocalDevelopmentEnvironment:\n\nProvides containerized dependencies (Vault, FerretDB, S3-compatible storage).\nSupports mock ERP implementations for testing.\nIncludes sample datasets for development.\n\nIntegrationTestHarness:\n\nFacilitates automated testing against mock ERPs.\nSupports scenario-based testing of the extraction process.\nIncludes performance benchmarking capabilities.",
    "crumbs": [
      "Archive",
      "Enhanced Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/enhanced-erp-extractor-design.html#pseudo-code-for-nomad-integration",
    "href": "archive/enhanced-erp-extractor-design.html#pseudo-code-for-nomad-integration",
    "title": "Enhanced Design Document for Distributor Data Extraction Ingestion",
    "section": "5 Pseudo-code for Nomad Integration",
    "text": "5 Pseudo-code for Nomad Integration\n/// &lt;summary&gt;\n/// Entry point for the ERP data extraction and ingestion process.\n/// Processes command-line arguments from Nomad and orchestrates the ETL workflow.\n/// &lt;/summary&gt;\nMain:\n    // Parse Nomad-supplied command-line arguments:\n    //   --erp-type, --client-id, --data-type\n    options = parseArguments([\"--erp-type\", \"--client-id\", \"--data-type\"])\n\n    // Build the host container with dependency injection configured\n    host = createHostBuilder().build()\n\n    // Register health checks for dependencies\n    host.registerHealthChecks()\n        .addCheck&lt;VaultHealthCheck&gt;(\"vault\")\n        .addCheck&lt;FerretDBHealthCheck&gt;(\"ferretdb\")\n        .addCheck&lt;S3HealthCheck&gt;(\"s3\")\n\n    // Setup OpenAPI documentation\n    host.configureOpenApi(options =&gt; {\n        options.Title = \"Distributor Data Extraction API\";\n        options.Version = \"v1\";\n        options.Description = \"API for extracting distributor data from various ERP systems\";\n    })\n\n    // Retrieve the ERPService from the DI container\n    erpService = host.getService(ERPService)\n\n    // Trigger the extraction ingestion process with the supplied parameters\n    erpService.ProcessERPData(options.erpType, options.clientId, options.dataType)\n\n/// &lt;summary&gt;\n/// Configures and builds the host with all necessary services.\n/// &lt;/summary&gt;\nCreateHostBuilder:\n    return HostBuilder()\n        .ConfigureServices(services =&gt;\n        {\n            // Configure Vault client with circuit breaker\n            services.AddSingleton&lt;IVaultClient&gt;(provider =&gt;\n            {\n                var vaultOptions = new VaultClientSettings(\n                    \"https://vault.example.com:8200\",\n                    new AppRoleAuthMethodInfo(roleId, secretId)\n                );\n                vaultOptions.RetrySettings = new RetrySettings { \n                    Enabled = true, \n                    MaxAttempts = 5,\n                    BackoffType = BackoffType.ExponentialWithJitter\n                };\n                return new VaultClient(vaultOptions);\n            });\n\n            // Configure FerretDB connection with bulkhead isolation\n            services.AddSingleton&lt;IMongoClient&gt;(provider =&gt;\n            {\n                var settings = MongoClientSettings.FromConnectionString(\n                    \"mongodb://ferretdb.example.com:27017\"\n                );\n                settings.RetryWrites = true;\n                settings.RetryReads = true;\n                settings.ServerSelectionTimeout = TimeSpan.FromSeconds(5);\n                settings.MaxConnectionPoolSize = 100;\n                return new MongoClient(settings);\n            });\n\n            // Configure feature flag service\n            services.AddSingleton&lt;IFeatureFlagService, FeatureFlagService&gt;();\n\n            // Register providers with caching decorators\n            services.AddSingleton&lt;ICredentialProvider, VaultCredentialProvider&gt;();\n            services.Decorate&lt;ICredentialProvider, CachedCredentialProviderDecorator&gt;();\n            \n            services.AddSingleton&lt;IConfigurationProvider, FerretDBConfigProvider&gt;();\n            \n            // Register data quality and validation services\n            services.AddSingleton&lt;IDataContractValidator, DataContractValidator&gt;();\n            services.AddSingleton&lt;IDataMaskingService, DataMaskingService&gt;();\n            \n            // Register data lineage service\n            services.AddSingleton&lt;IDataLineageService, DataLineageService&gt;();\n            \n            // Register registries\n            services.AddSingleton&lt;IERPRegistry, ERPRegistry&gt;();\n            services.AddSingleton&lt;IExtractorRegistry, ExtractorRegistry&gt;();\n            services.AddSingleton&lt;ITransformationRegistry, TransformationRegistry&gt;();\n            services.AddSingleton&lt;IUploaderRegistry, UploaderRegistry&gt;();\n            \n            // Register builders\n            services.AddSingleton&lt;IAPIRequestBuilder, APIRequestBuilder&gt;();\n            services.AddSingleton&lt;IDatabaseQueryBuilder, DatabaseQueryBuilder&gt;();\n            services.AddSingleton&lt;IAuthenticationBuilder, AuthenticationBuilder&gt;();\n            services.AddSingleton&lt;IExtractConfigBuilder, ExtractConfigBuilder&gt;();\n            \n            // Register S3 client for data upload\n            services.AddAWSService&lt;IAmazonS3&gt;();\n            \n            // Register core service with decorators for cross-cutting concerns\n            services.AddSingleton&lt;ERPService&gt;();\n            services.Decorate&lt;ERPService, MetricsERPServiceDecorator&gt;();\n            services.Decorate&lt;ERPService, DataQualityDecorator&gt;();\n            services.Decorate&lt;ERPService, EncryptionDecorator&gt;();\n            \n            // Configure HTTP clients with resilience policies using Polly\n            services.AddHttpClient(\"default\")\n                .AddTransientHttpErrorPolicy(builder =&gt; \n                    builder.WaitAndRetryAsync(\n                        retryCount: 3, \n                        sleepDurationProvider: retryAttempt =&gt; \n                            TimeSpan.FromSeconds(Math.Pow(2, retryAttempt)) + \n                            TimeSpan.FromMilliseconds(new Random().Next(0, 1000)), // Jitter\n                        onRetry: (outcome, timespan, retryAttempt, context) =&gt; {\n                            // Log retry attempt\n                            logger.LogWarning($\"Retry {retryAttempt} for {context.PolicyKey} after {timespan.TotalSeconds}s delay\");\n                        }\n                    ))\n                .AddCircuitBreakerPolicy(builder =&gt;\n                    builder.CircuitBreakerAsync(\n                        handledEventsAllowedBeforeBreaking: 5,\n                        durationOfBreak: TimeSpan.FromSeconds(30),\n                        onBreak: (outcome, breakDelay) =&gt; {\n                            logger.LogError($\"Circuit broken for {breakDelay.TotalSeconds}s!\");\n                        },\n                        onReset: () =&gt; {\n                            logger.LogInformation(\"Circuit reset!\");\n                        }\n                    ));\n                \n            // Add OpenTelemetry tracing\n            services.AddOpenTelemetryTracing(builder =&gt; {\n                builder\n                    .SetResourceBuilder(ResourceBuilder.CreateDefault().AddService(\"erp-extractor\"))\n                    .AddSource(\"erp-extractor\")\n                    .AddHttpClientInstrumentation()\n                    .AddMongoDBInstrumentation()\n                    .AddAspNetCoreInstrumentation()\n                    .AddJaegerExporter();\n            });\n        });\n\n/// &lt;summary&gt;\n/// Processes ERP data extraction and performs initial transformation.\n/// &lt;/summary&gt;\n/// &lt;param name=\"erpType\"&gt;The type of ERP system to extract from&lt;/param&gt;\n/// &lt;param name=\"clientId\"&gt;The client identifier&lt;/param&gt;\n/// &lt;param name=\"dataType\"&gt;The type of data to extract&lt;/param&gt;\n/// &lt;remarks&gt;\n/// This method handles both API and Database extraction modes. For API mode,\n/// it constructs appropriate API requests with authentication. For Database mode,\n/// it builds and executes SQL queries. In both cases, the extracted data is:\n/// 1. Minimally transformed (column standardization)\n/// 2. Validated against data contracts\n/// 3. Converted to Parquet format with compression\n/// 4. Uploaded to a pre-dropzone S3 bucket\n/// &lt;/remarks&gt;\nERPService.ProcessERPData(erpType, clientId, dataType):\n    Log \"Starting ETL extraction ingestion for client [clientId] using ERP [erpType]\"\n\n    /// &lt;summary&gt;Start metrics collection for this operation&lt;/summary&gt;\n    using (metricsTimer = MetricsService.StartTimer(\"erp_process_data\", \n                                                  { \"erp_type\": erpType, \"client_id\": clientId }))\n    using (tracer = TracingService.StartTrace(\"ProcessERPData\"))\n    {\n        /// &lt;summary&gt;Start data lineage tracking&lt;/summary&gt;\n        lineage = DataLineageService.StartLineageRecord(erpType, clientId, dataType)\n        \n        /// &lt;summary&gt;Check feature flags for enabled features&lt;/summary&gt;\n        bool useIncrementalExtract = FeatureFlagService.IsFeatureEnabled(\"IncrementalExtract\", clientId)\n        bool useCompression = FeatureFlagService.IsFeatureEnabled(\"Compression\", clientId)\n        bool useFieldEncryption = FeatureFlagService.IsFeatureEnabled(\"FieldEncryption\", clientId)\n    \n        /// &lt;summary&gt;Retrieve least-privilege credentials from HashiCorp Vault&lt;/summary&gt;\n        credentials = CredentialProvider.GetLeastPrivilegeCredentials(erpType, clientId, dataType)\n\n        /// &lt;summary&gt;Retrieve configuration from FerretDB&lt;/summary&gt;\n        erpConfig = ConfigurationProvider.GetConfiguration(erpType, clientId)\n\n        /// &lt;summary&gt;\n        /// Lookup common components via registries:\n        /// - ERP-specific factory (for connectors and jobs)\n        /// - Data extractor (for API or DB extraction)\n        /// - Data transformer (to standardize columns and convert to Parquet)\n        /// - Data validator (to validate data quality)\n        /// - Data uploader (to upload data to the pre-dropzone S3 bucket)\n        /// &lt;/summary&gt;\n        factory = ERPRegistry.GetFactory(erpType, clientId)\n        extractor = ExtractorRegistry.GetExtractor(erpType)\n        transformer = TransformationRegistry.GetStrategy(erpType, dataType)\n        validator = ValidatorRegistry.GetValidator(erpType, dataType)\n        uploader = UploaderRegistry.GetUploader(\"s3\")\n\n        /// &lt;summary&gt;Build extraction configuration based on mode&lt;/summary&gt;\n        extractConfig = ExtractConfigBuilder.New()\n            .ForERP(erpType)\n            .ForClient(clientId)\n            .ForDataType(dataType)\n            .UseIncrementalExtract(useIncrementalExtract && erpConfig.SupportsCDC)\n            .WithLastExtractTime(useIncrementalExtract ? GetLastExtractTime(erpType, clientId, dataType) : null)\n            .WithBatchSize(erpConfig.BatchSize)\n            .Build()\n\n        /// &lt;summary&gt;\n        /// Handle database extraction mode\n        /// Builds and executes SQL queries for direct database access\n        /// &lt;/summary&gt;\n        if erpConfig.AccessType == Database then:\n            queryBuilder = DatabaseQueryBuilder()\n                .ForERP(erpType)\n                .WithConnectionString(erpConfig.ConnectionString)\n                .WithSchema(erpConfig.Schema)\n                .WithTable(dataType + \"_table\")\n                .WithColumns(\"id\", \"created_at\", \"data\")\n                \n            if useIncrementalExtract && erpConfig.SupportsCDC:\n                queryBuilder.WithWhere(erpConfig.WatermarkColumn, \"&gt;\", extractConfig.LastExtractTime)\n            else:\n                queryBuilder.WithWhere(\"is_processed\", false)\n                \n            query = queryBuilder\n                .WithOrderBy(\"created_at\")\n                .WithLimit(erpConfig.BatchSize)\n                .WithCommandTimeout(erpConfig.TimeoutSeconds)\n                .Build()\n            \n            Log \"Executing database query: \" + query.GenerateSql()\n            \n            // Use bulkhead isolation for database connection\n            using (bulkhead = BulkheadPolicy.Execute(erpType + \"-database\", () =&gt; {\n                extractedData = extractor.ExtractFromDatabase(query, extractConfig)\n                return extractedData\n            }))\n\n        /// &lt;summary&gt;\n        /// Handle API extraction mode\n        /// Constructs and executes authenticated API requests with resilience\n        /// &lt;/summary&gt;\n        else:\n            authBuilder = AuthenticationBuilder()\n                .WithApiKey(credentials.ApiKey)\n                .WithClientId(credentials.ClientId)\n                .WithClientSecret(credentials.ClientSecret)\n                \n            if erpConfig.SupportsMutualTLS:\n                authBuilder.WithClientCertificate(credentials.ClientCertificate)\n                \n            auth = authBuilder.Build()\n            \n            requestBuilder = APIRequestBuilder()\n                .ForERP(erpType)\n                .WithEndpoint(erpConfig.BaseUrl + \"/api/v2/sales\")\n                .WithMethod(GET)\n                .WithAuthentication(auth)\n                .WithHeaders(erpConfig.RequiredHeaders)\n                \n            if useIncrementalExtract && erpConfig.SupportsCDC:\n                requestBuilder.WithQueryParameters({\n                    \"companyId\": erpConfig.CompanyId,\n                    \"warehouse\": erpConfig.WarehouseId,\n                    \"pageSize\": erpConfig.PageSize.toString(),\n                    \"changedSince\": extractConfig.LastExtractTime.toISOString()\n                })\n            else:\n                requestBuilder.WithQueryParameters({\n                    \"companyId\": erpConfig.CompanyId,\n                    \"warehouse\": erpConfig.WarehouseId,\n                    \"pageSize\": erpConfig.PageSize.toString()\n                })\n                \n            request = requestBuilder\n                .WithRetryPolicy(erpConfig.MaxRetries)\n                .WithTimeout(erpConfig.TimeoutSeconds)\n                .Build()\n            \n            Log \"Executing API request to \" + erpConfig.BaseUrl + \"/api/v2/sales\"\n            \n            // Execute with circuit breaker protection\n            extractedData = CircuitBreakerPolicy\n                .ForService(\"erp-api-\" + erpType)\n                .Execute(() =&gt; extractor.Extract(request, extractConfig))\n\n        /// &lt;summary&gt;Validate data against contract&lt;/summary&gt;\n        Log \"Validating extracted data against contract\"\n        validationResult = validator.Validate(extractedData)\n        \n        if !validationResult.IsValid:\n            // Handle data quality issues based on severity\n            if validationResult.HasCriticalIssues():\n                throw new DataContractException(\n                    \"Critical data quality issues detected: \" + \n                    validationResult.GetCriticalIssuesSummary()\n                )\n            else:\n                // Log warnings but continue processing\n                Log \"Warning: Data quality issues detected: \" + validationResult.GetIssuesSummary()\n                \n        /// &lt;summary&gt;Transform the extracted data with data lineage tracking&lt;/summary&gt;\n        Log \"Starting data transformation\"\n        transformedData = transformer.Transform(extractedData)\n        lineage.RecordTransformation(\"ColumnStandardization\", \"Standardized columns according to global data dictionary\")\n        \n        /// &lt;summary&gt;Apply field encryption for sensitive data if enabled&lt;/summary&gt;\n        if useFieldEncryption:\n            transformedData = EncryptionService.EncryptSensitiveFields(\n                transformedData, \n                GetSensitiveFieldsConfig(erpType, dataType)\n            )\n            lineage.RecordTransformation(\"FieldEncryption\", \"Encrypted sensitive fields\")\n            \n        /// &lt;summary&gt;Apply masking for non-production environments&lt;/summary&gt;\n        if environmentType != Production:\n            transformedData = DataMaskingService.ApplyMasking(\n                transformedData,\n                GetDataMaskingConfig(erpType, dataType)\n            )\n            lineage.RecordTransformation(\"DataMasking\", \"Applied data masking for non-production environment\")\n        \n        /// &lt;summary&gt;\n        /// Configure and execute the S3 upload operation\n        /// Data is stored in a pre-dropzone bucket with standardized path structure\n        /// &lt;/summary&gt;\n        currentTimestamp = getCurrentTimestamp()\n        \n        uploadConfig = new UploadConfiguration(\n                        Bucket: \"erp-data-\" + clientId,\n                        Key: erpType + \"/\" + dataType + \"/\" + currentTimestamp + \"/data.parquet\",\n                        Format: Parquet,\n                        Compress: useCompression,\n                        CompressionType: useCompression ? \"SNAPPY\" : null,\n                        Metadata: {\n                           \"erp_type\": erpType,\n                           \"client_id\": clientId,\n                           \"data_type\": dataType,\n                           \"extract_timestamp\": currentTimestamp,\n                           \"lineage_id\": lineage.Id,\n                           \"extract_mode\": useIncrementalExtract ? \"incremental\" : \"full\",\n                           \"record_count\": transformedData.Count,\n                           \"schema_version\": \"1.2\"\n                        }\n                    )\n        \n        Log \"Starting data upload to S3 pre-dropzone\"\n        uploadResult = uploader.Upload(transformedData, uploadConfig)\n        \n        /// &lt;summary&gt;Update data catalog with metadata&lt;/summary&gt;\n        CatalogService.UpdateDatasetMetadata(\n            datasetId: erpType + \"-\" + clientId + \"-\" + dataType,\n            metadata: {\n                \"lastUpdated\": currentTimestamp,\n                \"recordCount\": transformedData.Count,\n                \"fileLocation\": uploadResult.Location,\n                \"fileSize\": uploadResult.Size,\n                \"schemaVersion\": \"1.2\",\n                \"lineageId\": lineage.Id\n            }\n        )\n        \n        /// &lt;summary&gt;Complete lineage record&lt;/summary&gt;\n        lineage.SetDestination(uploadResult.Location)\n        lineage.CompleteLineageRecord()\n\n        /// &lt;summary&gt;Update last extract time for incremental extracts&lt;/summary&gt;\n        if useIncrementalExtract:\n            StoreLastExtractTime(erpType, clientId, dataType, currentTimestamp)\n        \n        Log \"Extraction ingestion process completed successfully\"\n    }",
    "crumbs": [
      "Archive",
      "Enhanced Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/enhanced-erp-extractor-design.html#advanced-error-handling-and-recovery",
    "href": "archive/enhanced-erp-extractor-design.html#advanced-error-handling-and-recovery",
    "title": "Enhanced Design Document for Distributor Data Extraction Ingestion",
    "section": "6 Advanced Error Handling and Recovery",
    "text": "6 Advanced Error Handling and Recovery\n\nError Classification:\n\nCategorizes errors as transient or persistent.\nApplies different recovery strategies based on error type.\nRecords error patterns for proactive monitoring.\n\nSelf-Healing Procedures:\n\nCommon Issue Resolution:\n\nAutomatic token renewal for expired credentials.\nConnection pool refresh for stale connections.\nTemporary file cleanup for storage-related issues.\n\nDegraded Mode Operation:\n\nFalls back to full extract if incremental extract fails.\nDisables optional features during high load.\nImplements progressive backoff for system recovery.\n\n\nComprehensive Logging:\n\nStructured logging with correlation IDs across components.\nContext-enriched log entries for easier debugging.\nLog level adjustment based on operation criticality.\nIntegration with log aggregation systems.",
    "crumbs": [
      "Archive",
      "Enhanced Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/enhanced-erp-extractor-design.html#security-and-compliance",
    "href": "archive/enhanced-erp-extractor-design.html#security-and-compliance",
    "title": "Enhanced Design Document for Distributor Data Extraction Ingestion",
    "section": "7 Security and Compliance",
    "text": "7 Security and Compliance\n\nLeast Privilege Access:\n\nDynamic generation of operation-specific credentials.\nShort-lived tokens with minimal required permissions.\nCredential scoping based on operation context.\nRole rotation and separation of duties.\n\nData Protection:\n\nField-level encryption for sensitive data.\nData masking for non-production environments.\nSecure credential management with HashiCorp Vault.\nMutual TLS for secure service communication.\n\nAudit and Compliance:\n\nComprehensive data lineage tracking.\nAccess and operation audit logging.\nCompliance validation against regulatory requirements.\nRegular security posture assessment.",
    "crumbs": [
      "Archive",
      "Enhanced Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "archive/enhanced-erp-extractor-design.html#development-and-operations-support",
    "href": "archive/enhanced-erp-extractor-design.html#development-and-operations-support",
    "title": "Enhanced Design Document for Distributor Data Extraction Ingestion",
    "section": "8 Development and Operations Support",
    "text": "8 Development and Operations Support\n\nLocal Development Environment:\n\nDocker Compose setup with all dependencies.\nMock ERPs for integration testing.\nConfiguration templates for different scenarios.\nDevelopment tools for data visualization and debugging.\n\nDeployment and Infrastructure:\n\nGitOps-based deployment pipelines.\nInfrastructure as Code for all components.\nCanary deployment support for risk mitigation.\nBlue-green deployment capability for zero downtime.\n\nDocumentation and Knowledge Sharing:\n\nOpenAPI documentation for all service interfaces.\nArchitecture decision records (ADRs) for design choices.\nRunbooks for common operational procedures.\nAutomated documentation generation from code.",
    "crumbs": [
      "Archive",
      "Enhanced Design Document for Distributor Data Extraction Ingestion"
    ]
  },
  {
    "objectID": "dpp/workflows/database-extraction-workflow.html#process-steps",
    "href": "dpp/workflows/database-extraction-workflow.html#process-steps",
    "title": "Database Extraction Workflow",
    "section": "2 Process Steps",
    "text": "2 Process Steps\n\n2.1 ERP Extraction Triggered\nThe workflow begins when an extraction request is initiated. This could happen:\n\nOn a scheduled basis (e.g., nightly extractions)\nOn-demand through user-initiated requests\nAs part of a larger data processing workflow\n\n\n\n2.2 Get Database Configuration\nThe system retrieves the necessary database configuration for the specified ERP system and client:\n\nDatabase connection strings\nSchema information\nTable structures\nConnection pool settings\nQuery timeout configurations\n\nThe system securely accesses the ERP database by:\n\nRetrieving secure credentials from the vault\nEstablishing a secure database connection\nVerifying connection success\nConfiguring connection parameters (transaction isolation, timeouts)\n\n\n\n2.3 Execute Database Queries\nWith a successful connection established, the system executes the appropriate queries to extract the required data:\n\nSales orders\nSales order details\nInventory spec fields\nCustomer details\nVendor details\nProduct spec fields\n\n\n\n2.4 Transform to Standard Columns\nThe raw data from the database is transformed to follow our standardized data model:\n\nColumn names are mapped to our standard naming conventions\nBasic data quality checks are performed\n\n\n\n2.5 Save to Parquet\nThe transformed data is converted to Parquet format:\n\nEfficient columnar storage format\nCompressed to save space\nOptimized for analytical queries\nSchema is preserved with appropriate data types\n\n\n\n2.6 Push to S3\nThe Parquet files are uploaded to our S3 storage:\n\nOrganized by client and data type\nStored with appropriate metadata\nSecured with proper access controls\nMade available for downstream processing",
    "crumbs": [
      "Distributor Partner Program",
      "Workflows",
      "Database Extraction Workflow"
    ]
  }
]